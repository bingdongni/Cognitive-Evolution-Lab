#!/usr/bin/env python3
"""
Cognitive Evolution Lab - ä¸»è¦æ¼”ç¤ºè„šæœ¬
ä½œè€…: bingdongni
ç‰ˆæœ¬: v1.0.0

æ­¤è„šæœ¬æ¼”ç¤ºè®¤çŸ¥è¿›åŒ–å®éªŒå®¤çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. å…­ç§è®¤çŸ¥èƒ½åŠ›æµ‹è¯•ï¼ˆè®°å¿†ã€æ¨ç†ã€åˆ›é€ åŠ›ã€è§‚å¯ŸåŠ›ã€æ³¨æ„åŠ›ã€æƒ³è±¡åŠ›ï¼‰
2. ååŒè¿›åŒ–å®éªŒ
3. è®¤çŸ¥è®¤çŸ¥ä¸»ä½“äº¤äº’
4. å®æ—¶æ•°æ®å¯è§†åŒ–

è¿è¡Œæ–¹å¼ï¼š
    python scripts/demo.py [--mode demo|cognitive|evolution|full] [--port PORT]
"""

import asyncio
import logging
import argparse
import json
import time
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent.parent))

from src.cognitive_models import CognitiveAgent, MemoryType, ReasoningType
from src.world_simulator import VirtualWorld
from src.evolution_engine import EvolutionEngine
from src.utils import setup_logging, load_config


class CognitiveDemo:
    """
    è®¤çŸ¥æ¼”ç¤ºç±» - å±•ç¤ºå…­ç§è®¤çŸ¥èƒ½åŠ›çš„ç»¼åˆæµ‹è¯•
    """
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–æ¼”ç¤ºç¯å¢ƒ
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        self.config = load_config(config_path) if config_path else self._get_default_config()
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(self.config)
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.cognitive_è®¤çŸ¥ä¸»ä½“ = None
        self.world_simulator = None
        self.evolution_engine = None
        
        # æ¼”ç¤ºç»“æœå­˜å‚¨
        self.demo_results = {
            'cognitive_tests': {},
            'evolution_results': {},
            'performance_metrics': {},
            'demo_timestamp': time.time()
        }
        
        self.logger.info("ğŸ¯ è®¤çŸ¥æ¼”ç¤ºç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ¼”ç¤ºé…ç½®"""
        return {
            'global': {
                'debug': True,
                'log_level': 'INFO',
                'save_path': './demo_results'
            },
            'cognitive_models': {
                'vocab_size': 1000,
                'embed_dim': 256,
                'hidden_dim': 512,
                'learning_rate': 0.001
            },
            'world_simulator': {
                'environment_size': [50, 50, 20],
                'max_objects': 100
            },
            'evolution_engine': {
                'population_size': 20,
                'generations': 10
            }
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–æ¼”ç¤ºç»„ä»¶"""
        self.logger.info("ğŸ”§ åˆå§‹åŒ–æ¼”ç¤ºç»„ä»¶...")
        
        try:
            # åˆå§‹åŒ–è®¤çŸ¥è®¤çŸ¥ä¸»ä½“
            self.cognitive_è®¤çŸ¥ä¸»ä½“ = CognitiveAgent(
                config=self.config['cognitive_models']
            )
            await self.cognitive_è®¤çŸ¥ä¸»ä½“.initialize()
            self.logger.info("âœ… è®¤çŸ¥è®¤çŸ¥ä¸»ä½“åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–ä¸–ç•Œæ¨¡æ‹Ÿå™¨
            self.world_simulator = VirtualWorld(
                config=self.config['world_simulator']
            )
            await self.world_simulator.initialize()
            self.logger.info("âœ… ä¸–ç•Œæ¨¡æ‹Ÿå™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–è¿›åŒ–å¼•æ“
            self.evolution_engine = EvolutionEngine(
                config=self.config['evolution_engine']
            )
            await self.evolution_engine.initialize()
            self.logger.info("âœ… è¿›åŒ–å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def run_memory_demo(self) -> Dict[str, Any]:
        """
        æ¼”ç¤ºè®°å¿†ç³»ç»ŸåŠŸèƒ½
        
        å±•ç¤ºæƒ…æ™¯è®°å¿†ã€è¯­ä¹‰è®°å¿†ã€å·¥ä½œè®°å¿†å’Œç¨‹åºè®°å¿†çš„å­˜å‚¨ä¸æ£€ç´¢
        """
        self.logger.info("ğŸ§  å¼€å§‹è®°å¿†ç³»ç»Ÿæ¼”ç¤º...")
        
        memory_results = {
            'test_type': 'memory_system',
            'episodic_memory': {},
            'semantic_memory': {},
            'working_memory': {},
            'procedural_memory': {},
            'retention_test': {},
            'association_test': {}
        }
        
        # 1. æƒ…æ™¯è®°å¿†æµ‹è¯• - å­˜å‚¨ä¸ªäººç»å†
        self.logger.info("ğŸ“ æµ‹è¯•æƒ…æ™¯è®°å¿†...")
        episodic_memories = [
            "ä»Šå¤©å­¦ä¼šäº†ä¸€ä¸ªæ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•",
            "å’ŒåŒäº‹è®¨è®ºäº†è®¤çŸ¥ç§‘å­¦çš„æœ€æ–°è¿›å±•", 
            "å®Œæˆäº†ä¸€ä¸ªå¤æ‚çš„ç¼–ç¨‹ä»»åŠ¡",
            "è§‚çœ‹äº†å…³äºäººå·¥æ™ºèƒ½çš„TEDæ¼”è®²",
            "è¯»äº†ä¸€ç¯‡å…³äºç¥ç»ç½‘ç»œçš„è®ºæ–‡"
        ]
        
        for i, memory in enumerate(episodic_memories):
            await self.cognitive_è®¤çŸ¥ä¸»ä½“.store_memory(
                content=memory,
                memory_type=MemoryType.EPISODIC,
                strength=0.8 + i * 0.05
            )
        
        memory_results['episodic_memory']['stored_count'] = len(episodic_memories)
        
        # 2. è¯­ä¹‰è®°å¿†æµ‹è¯• - å­˜å‚¨äº‹å®çŸ¥è¯†
        self.logger.info("ğŸ“š æµ‹è¯•è¯­ä¹‰è®°å¿†...")
        semantic_facts = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ",
            "è®¤çŸ¥ç§‘å­¦ç ”ç©¶å¿ƒæ™ºå’Œæ€ç»´è¿‡ç¨‹",
            "è¿›åŒ–ç®—æ³•æ¨¡æ‹Ÿè‡ªç„¶é€‰æ‹©è¿‡ç¨‹",
            "æœºå™¨å­¦ä¹ ä½¿è®¡ç®—æœºèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ "
        ]
        
        for fact in semantic_facts:
            await self.cognitive_è®¤çŸ¥ä¸»ä½“.store_memory(
                content=fact,
                memory_type=MemoryType.SEMANTIC,
                strength=0.9
            )
        
        memory_results['semantic_memory']['stored_count'] = len(semantic_facts)
        
        # 3. å·¥ä½œè®°å¿†æµ‹è¯• - çŸ­æœŸä¿¡æ¯å¤„ç†
        self.logger.info("âš¡ æµ‹è¯•å·¥ä½œè®°å¿†...")
        working_tasks = [
            "è®°ä½æ•°å­—åºåˆ—: 3-7-1-9-2",
            "å¿ƒç®—: 25 Ã— 4 = ?",
            "å€’èƒŒå­—æ¯: A-D-G-J",
            "åŒä»»åŠ¡å¤„ç†: åŒæ—¶è®°å¿†é¢œè‰²å’Œå½¢çŠ¶"
        ]
        
        for i, task in enumerate(working_tasks):
            await self.cognitive_è®¤çŸ¥ä¸»ä½“.store_memory(
                content=task,
                memory_type=MemoryType.WORKING,
                strength=0.7
            )
        
        memory_results['working_memory']['task_count'] = len(working_tasks)
        
        # 4. ç¨‹åºè®°å¿†æµ‹è¯• - æŠ€èƒ½å’Œä¹ æƒ¯
        self.logger.info("ğŸ”§ æµ‹è¯•ç¨‹åºè®°å¿†...")
        procedural_skills = [
            "å¦‚ä½•éª‘è‡ªè¡Œè½¦",
            "æ‰“å­—çš„åŸºæœ¬æ‰‹åŠ¿",
            "è§£å†³é—®é¢˜çš„æ­¥éª¤æµç¨‹",
            "å­¦ä¹ æ–°æŠ€èƒ½çš„æ–¹æ³•"
        ]
        
        for skill in procedural_skills:
            await self.cognitive_è®¤çŸ¥ä¸»ä½“.store_memory(
                content=skill,
                memory_type=MemoryType.PROCEDURAL,
                strength=0.95
            )
        
        memory_results['procedural_memory']['skill_count'] = len(procedural_skills)
        
        # 5. è®°å¿†æ£€ç´¢æµ‹è¯•
        self.logger.info("ğŸ” æµ‹è¯•è®°å¿†æ£€ç´¢...")
        retrieval_tests = [
            ("å­¦ä¹ ", MemoryType.SEMANTIC, 0.6),
            ("ç®—æ³•", MemoryType.SEMANTIC, 0.7),
            ("è®°å¿†", MemoryType.EPISODIC, 0.5),
            ("æŠ€èƒ½", MemoryType.PROCEDURAL, 0.6)
        ]
        
        retrieval_results = []
        for query, mem_type, threshold in retrieval_tests:
            retrieved = await self.cognitive_è®¤çŸ¥ä¸»ä½“.retrieve_memory(
                query=query,
                memory_type=mem_type,
                threshold=threshold
            )
            retrieval_results.append({
                'query': query,
                'type': mem_type.value,
                'retrieved_count': len(retrieved),
                'threshold': threshold
            })
        
        memory_results['retrieval_test'] = retrieval_results
        
        # 6. è®°å¿†ä¿ç•™æµ‹è¯•
        retention_test = await self.cognitive_è®¤çŸ¥ä¸»ä½“.test_memory_retention()
        memory_results['retention_test'] = retention_test
        
        # è®¡ç®—è®°å¿†æ€§èƒ½è¯„åˆ†
        total_stored = (len(episodic_memories) + len(semantic_facts) + 
                       len(working_tasks) + len(procedural_skills))
        avg_retrieval = sum(r['retrieved_count'] for r in retrieval_results) / len(retrieval_results)
        retrieval_accuracy = min(1.0, avg_retrieval / 3.0)  # å‡è®¾æœŸæœ›æ£€ç´¢3ä¸ªç›¸å…³è®°å¿†
        
        memory_results['performance_score'] = (retrieval_accuracy + retention_test['retention_score']) / 2
        memory_results['total_memories_stored'] = total_stored
        
        self.logger.info(f"âœ… è®°å¿†ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼Œæ€§èƒ½è¯„åˆ†: {memory_results['performance_score']:.2f}")
        return memory_results
    
    async def run_reasoning_demo(self) -> Dict[str, Any]:
        """
        æ¼”ç¤ºæ¨ç†ç³»ç»ŸåŠŸèƒ½
        
        å±•ç¤ºæ¼”ç»æ¨ç†ã€å½’çº³æ¨ç†ã€æº¯å› æ¨ç†å’Œç±»æ¯”æ¨ç†
        """
        self.logger.info("ğŸ§© å¼€å§‹æ¨ç†ç³»ç»Ÿæ¼”ç¤º...")
        
        reasoning_results = {
            'test_type': 'reasoning_system',
            'deductive_reasoning': {},
            'inductive_reasoning': {},
            'abductive_reasoning': {},
            'analogical_reasoning': {},
            'overall_performance': {}
        }
        
        # 1. æ¼”ç»æ¨ç†æµ‹è¯• - ä»ä¸€èˆ¬åˆ°ç‰¹æ®Š
        self.logger.info("ğŸ”¬ æµ‹è¯•æ¼”ç»æ¨ç†...")
        deductive_cases = [
            {
                'premises': ["æ‰€æœ‰ç§‘å­¦å®¶éƒ½å¾ˆæœ‰å¥½å¥‡å¿ƒ", "çˆ±å› æ–¯å¦æ˜¯ç§‘å­¦å®¶"],
                'expected': "çˆ±å› æ–¯å¦å¾ˆæœ‰å¥½å¥‡å¿ƒ",
                'description': "ä¸‰æ®µè®ºæ¼”ç»æ¨ç†"
            },
            {
                'premises': ["å¦‚æœä¸‹é›¨ï¼Œåœ°é¢ä¼šæ¹¿", "æ­£åœ¨ä¸‹é›¨"],
                'expected': "åœ°é¢ä¼šæ¹¿",
                'description': "æ¡ä»¶æ¨ç†"
            },
            {
                'premises': ["æ‰€æœ‰é¸Ÿç±»éƒ½æœ‰ç¾½æ¯›", "ä¼é¹…æ˜¯é¸Ÿç±»"],
                'expected': "ä¼é¹…æœ‰ç¾½æ¯›",
                'description': "åˆ†ç±»æ¨ç†"
            }
        ]
        
        deductive_performance = []
        for case in deductive_cases:
            reasoning_chain = await self.cognitive_è®¤çŸ¥ä¸»ä½“.reason(
                premises=case['premises'],
                reasoning_type=ReasoningType.DEDUCTIVE
            )
            deductive_performance.append({
                'premises': case['premises'],
                'conclusion': reasoning_chain.conclusion,
                'confidence': reasoning_chain.confidence,
                'expected': case['expected'],
                'description': case['description']
            })
        
        reasoning_results['deductive_reasoning'] = {
            'test_cases': deductive_performance,
            'average_confidence': sum(c['confidence'] for c in deductive_performance) / len(deductive_performance)
        }
        
        # 2. å½’çº³æ¨ç†æµ‹è¯• - ä»ç‰¹æ®Šåˆ°ä¸€èˆ¬
        self.logger.info("ğŸ“Š æµ‹è¯•å½’çº³æ¨ç†...")
        inductive_cases = [
            {
                'observations': ["è§‚å¯Ÿåˆ°å¤©é¹…1æ˜¯ç™½çš„", "è§‚å¯Ÿåˆ°å¤©é¹…2æ˜¯ç™½çš„", "è§‚å¯Ÿåˆ°å¤©é¹…3æ˜¯ç™½çš„"],
                'hypothesis': "æ‰€æœ‰å¤©é¹…éƒ½æ˜¯ç™½çš„",
                'description': "è§‚å¯Ÿå½’çº³"
            },
            {
                'observations': ["ä¸‹é›¨å¤©äº¤é€šå¾ˆå µ", "ä¸‹é›ªå¤©äº¤é€šå¾ˆå µ", "åˆ®é£å¤©äº¤é€šå¾ˆå µ"],
                'hypothesis': "æ¶åŠ£å¤©æ°”å¯¼è‡´äº¤é€šæ‹¥å µ",
                'description': "å› æœå½’çº³"
            }
        ]
        
        inductive_performance = []
        for case in inductive_cases:
            reasoning_chain = await self.cognitive_è®¤çŸ¥ä¸»ä½“.reason(
                premises=case['observations'],
                reasoning_type=ReasoningType.INDUCTIVE
            )
            inductive_performance.append({
                'observations': case['observations'],
                'conclusion': reasoning_chain.conclusion,
                'confidence': reasoning_chain.confidence,
                'hypothesis': case['hypothesis'],
                'description': case['description']
            })
        
        reasoning_results['inductive_reasoning'] = {
            'test_cases': inductive_performance,
            'average_confidence': sum(c['confidence'] for c in inductive_performance) / len(inductive_performance)
        }
        
        # 3. æº¯å› æ¨ç†æµ‹è¯• - æœ€ä½³è§£é‡Šæ¨ç†
        self.logger.info("ğŸ” æµ‹è¯•æº¯å› æ¨ç†...")
        abductive_cases = [
            {
                'observations': ["è‰æ˜¯æ¹¿çš„"],
                'possible_explanations': ["ä¸‹é›¨äº†", "æœ‰äººæ´’äº†æ°´", "éœ²æ°´å‡ç»“"],
                'best_explanation': "ä¸‹é›¨äº†",
                'description': "ç°è±¡è§£é‡Š"
            },
            {
                'observations': ["è¡—ä¸Šæ¹¿äº†"],
                'possible_explanations': ["ä¸‹é›¨äº†", "æ´’æ°´è½¦ç»è¿‡", "æ°´ç®¡çˆ†è£‚"],
                'best_explanation': "ä¸‹é›¨äº†",
                'description': "ç»¼åˆæ¨ç†"
            }
        ]
        
        abductive_performance = []
        for case in abductive_cases:
            reasoning_chain = await self.cognitive_è®¤çŸ¥ä¸»ä½“.reason(
                premises=[case['observations']],
                reasoning_type=ReasoningType.ABDUCTIVE
            )
            abductive_performance.append({
                'observations': case['observations'],
                'possible_explanations': case['possible_explanations'],
                'conclusion': reasoning_chain.conclusion,
                'confidence': reasoning_chain.confidence,
                'best_explanation': case['best_explanation']
            })
        
        reasoning_results['abductive_reasoning'] = {
            'test_cases': abductive_performance,
            'average_confidence': sum(c['confidence'] for c in abductive_performance) / len(abductive_performance)
        }
        
        # 4. ç±»æ¯”æ¨ç†æµ‹è¯• - ç»“æ„æ˜ å°„
        self.logger.info("ğŸ”— æµ‹è¯•ç±»æ¯”æ¨ç†...")
        analogical_patterns = [
            {
                'source': "å¤ªé˜³ç³»çš„è¡Œæ˜Ÿå›´ç»•å¤ªé˜³è¿è½¬",
                'target_domain': "åŸå­ç»“æ„",
                'analogy': "ç”µå­å›´ç»•åŸå­æ ¸è¿è½¬",
                'mapping_quality': 0.8
            },
            {
                'source': "è¡€æ¶²å¾ªç¯ç³»ç»Ÿ",
                'target_domain': "è®¡ç®—æœºç½‘ç»œ",
                'analogy': "æ•°æ®åŒ…åœ¨ç½‘ç»œä¸­ä¼ è¾“",
                'mapping_quality': 0.7
            }
        ]
        
        reasoning_results['analogical_reasoning'] = {
            'analogies': analogical_patterns,
            'average_quality': sum(a['mapping_quality'] for a in analogical_patterns) / len(analogical_patterns)
        }
        
        # è®¡ç®—ç»¼åˆæ¨ç†æ€§èƒ½
        all_confidences = []
        all_confidences.extend([c['confidence'] for c in deductive_performance])
        all_confidences.extend([c['confidence'] for c in inductive_performance])
        all_confidences.extend([c['confidence'] for c in abductive_performance])
        
        reasoning_results['overall_performance'] = {
            'total_tests': len(all_confidences),
            'average_confidence': sum(all_confidences) / len(all_confidences),
            'deductive_score': reasoning_results['deductive_reasoning']['average_confidence'],
            'inductive_score': reasoning_results['inductive_reasoning']['average_confidence'],
            'abductive_score': reasoning_results['abductive_reasoning']['average_confidence'],
            'analogical_score': reasoning_results['analogical_reasoning']['average_quality']
        }
        
        self.logger.info(f"âœ… æ¨ç†ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼Œå¹³å‡ç½®ä¿¡åº¦: {reasoning_results['overall_performance']['average_confidence']:.2f}")
        return reasoning_results
    
    async def run_creativity_demo(self) -> Dict[str, Any]:
        """
        æ¼”ç¤ºåˆ›é€ åŠ›ç³»ç»ŸåŠŸèƒ½
        
        å±•ç¤ºå‘æ•£æ€ç»´ã€æ”¶æ•›æ€ç»´ã€åˆ›æ„æ€ç»´å’Œæƒ³è±¡åŠ›
        """
        self.logger.info("ğŸ¨ å¼€å§‹åˆ›é€ åŠ›ç³»ç»Ÿæ¼”ç¤º...")
        
        creativity_results = {
            'test_type': 'creativity_system',
            'divergent_thinking': {},
            'convergent_thinking': {},
            'creative_problem_solving': {},
            'novelty_evaluation': {}
        }
        
        # 1. å‘æ•£æ€ç»´æµ‹è¯• - ç”Ÿæˆå¤šç§è§£å†³æ–¹æ¡ˆ
        self.logger.info("ğŸŒŸ æµ‹è¯•å‘æ•£æ€ç»´...")
        divergent_tasks = [
            {
                'prompt': "åˆ—ä¸¾æ‰€æœ‰å¯ä»¥ç”¨æ¯å­åšçš„äº‹æƒ…",
                'context': "æ—¥å¸¸ç”Ÿæ´»",
                'expected_variety': 15
            },
            {
                'prompt': "æƒ³å‡ºæ‰€æœ‰å¯èƒ½çš„äº¤é€šæ–¹å¼",
                'context': "åŸå¸‚è§„åˆ’",
                'expected_variety': 10
            },
            {
                'prompt': "æå‡ºè§£å†³ç¯å¢ƒæ±¡æŸ“çš„åˆ›æ–°æ–¹æ³•",
                'context': "ç¯å¢ƒä¿æŠ¤",
                'expected_variety': 12
            }
        ]
        
        divergent_scores = []
        for task in divergent_tasks:
            # æ¨¡æ‹Ÿå‘æ•£æ€ç»´ç»“æœ
            creative_output = await self.cognitive_è®¤çŸ¥ä¸»ä½“.generate_creative_output(
                context=task['context'],
                style="divergent"
            )
            
            # è¯„ä¼°åˆ›æ„å¤šæ ·æ€§ï¼ˆç®€åŒ–ä¸ºç”Ÿæˆå…ƒç´ çš„è®¡æ•°ï¼‰
            diversity_score = min(1.0, len(creative_output.get('creative_text', '')) / 50)
            
            divergent_scores.append({
                'task': task['prompt'],
                'context': task['context'],
                'diversity_score': diversity_score,
                'creativity_score': creative_output['creativity_score'],
                'expected_variety': task['expected_variety']
            })
        
        creativity_results['divergent_thinking'] = {
            'tasks': divergent_scores,
            'average_diversity': sum(s['diversity_score'] for s in divergent_scores) / len(divergent_scores),
            'average_creativity': sum(s['creativity_score'] for s in divergent_scores) / len(divergent_scores)
        }
        
        # 2. æ”¶æ•›æ€ç»´æµ‹è¯• - é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ
        self.logger.info("ğŸ¯ æµ‹è¯•æ”¶æ•›æ€ç»´...")
        convergent_tasks = [
            {
                'problem': "å¦‚ä½•åœ¨æœ‰é™é¢„ç®—ä¸‹æé«˜å·¥ä½œæ•ˆç‡",
                'solutions': ["ä½¿ç”¨å…è´¹è½¯ä»¶", "ä¼˜åŒ–å·¥ä½œæµç¨‹", "å‡å°‘ä¼šè®®æ—¶é—´", "è‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡"],
                'criteria': ["æˆæœ¬", "æ•ˆæœ", "å¯å®æ–½æ€§"]
            },
            {
                'problem': "å¦‚ä½•è®©æ›´å¤šäººå‚ä¸ç¯ä¿æ´»åŠ¨",
                'solutions': ["æ•™è‚²å®£ä¼ ", "æ¸¸æˆåŒ–æœºåˆ¶", "å¥–åŠ±åˆ¶åº¦", "ç¤¾äº¤åª’ä½“æ¨å¹¿"],
                'criteria': ["å‚ä¸åº¦", "æŒç»­æ€§", "å½±å“åŠ›"]
            }
        ]
        
        convergence_scores = []
        for task in convergent_tasks:
            # æ¨¡æ‹Ÿæ”¶æ•›æ€ç»´è¯„ä¼°
            creative_output = await self.cognitive_è®¤çŸ¥ä¸»ä½“.generate_creative_output(
                context=f"æ”¶æ•›æ€ç»´ï¼š{task['problem']}",
                style="convergent"
            )
            
            convergence_score = creative_output['creativity_score']
            
            convergence_scores.append({
                'problem': task['problem'],
                'solutions_count': len(task['solutions']),
                'convergence_score': convergence_score,
                'evaluation_criteria': task['criteria']
            })
        
        creativity_results['convergent_thinking'] = {
            'tasks': convergence_scores,
            'average_convergence': sum(s['convergence_score'] for s in convergence_scores) / len(convergence_scores)
        }
        
        # 3. åˆ›æ„é—®é¢˜è§£å†³æµ‹è¯•
        self.logger.info("ğŸ’¡ æµ‹è¯•åˆ›æ„é—®é¢˜è§£å†³...")
        problem_solving_cases = [
            {
                'scenario': "è®¾è®¡ä¸€ä¸ªæ™ºèƒ½å®¶å±…ç³»ç»Ÿ",
                'constraints': ["æˆæœ¬æ§åˆ¶", "ç”¨æˆ·å‹å¥½", "èŠ‚èƒ½ç¯ä¿"],
                'creativity_aspects': ["æ–°é¢–æ€§", "å®ç”¨æ€§", "å¯è¡Œæ€§"]
            },
            {
                'scenario': "åˆ›é€ æ–°çš„å­¦ä¹ æ–¹å¼",
                'constraints': ["æé«˜å‚ä¸åº¦", "ä¸ªæ€§åŒ–", "å¯æ‰©å±•"],
                'creativity_aspects': ["åˆ›æ–°æ€§", "æ•ˆæœæ€§", "æ¨å¹¿æ€§"]
            }
        ]
        
        problem_solving_scores = []
        for case in problem_solving_cases:
            creative_output = await self.cognitive_è®¤çŸ¥ä¸»ä½“.generate_creative_output(
                context=case['scenario'],
                style="problem_solving"
            )
            
            problem_solving_scores.append({
                'scenario': case['scenario'],
                'constraints': case['constraints'],
                'creativity_aspects': case['creativity_aspects'],
                'solution_quality': creative_output['creativity_score'],
                'creative_text': creative_output['creative_text']
            })
        
        creativity_results['creative_problem_solving'] = {
            'cases': problem_solving_scores,
            'average_quality': sum(c['solution_quality'] for c in problem_solving_scores) / len(problem_solving_scores)
        }
        
        # 4. æ–°é¢–æ€§è¯„ä¼°
        self.logger.info("ğŸ”¬ æµ‹è¯•æ–°é¢–æ€§è¯„ä¼°...")
        novelty_assessments = [
            {
                'concept': "ä¼šè¯´è¯çš„æ¤ç‰©",
                'existing_knowledge': "æ¤ç‰©ä¸èƒ½è¯´è¯",
                'novelty_score': 0.9
            },
            {
                'concept': "æ—¶é—´æ—…è¡Œ",
                'existing_knowledge': "æ—¶é—´åªèƒ½å‘å‰æµé€",
                'novelty_score': 0.85
            },
            {
                'concept': "é‡å­è®¡ç®—",
                'existing_knowledge': "è®¡ç®—ä½¿ç”¨äºŒè¿›åˆ¶",
                'novelty_score': 0.75
            }
        ]
        
        creativity_results['novelty_evaluation'] = {
            'assessments': novelty_assessments,
            'average_novelty': sum(n['novelty_score'] for n in novelty_assessments) / len(novelty_assessments)
        }
        
        # è®¡ç®—ç»¼åˆåˆ›é€ åŠ›è¯„åˆ†
        creativity_results['overall_creativity'] = {
            'divergent_score': creativity_results['divergent_thinking']['average_diversity'],
            'convergent_score': creativity_results['convergent_thinking']['average_convergence'],
            'problem_solving_score': creativity_results['creative_problem_solving']['average_quality'],
            'novelty_score': creativity_results['novelty_evaluation']['average_novelty'],
            'composite_score': (
                creativity_results['divergent_thinking']['average_diversity'] +
                creativity_results['convergent_thinking']['average_convergence'] +
                creativity_results['creative_problem_solving']['average_quality'] +
                creativity_results['novelty_evaluation']['average_novelty']
            ) / 4
        }
        
        self.logger.info(f"âœ… åˆ›é€ åŠ›ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼Œç»¼åˆè¯„åˆ†: {creativity_results['overall_creativity']['composite_score']:.2f}")
        return creativity_results
    
    async def run_observation_demo(self) -> Dict[str, Any]:
        """
        æ¼”ç¤ºè§‚å¯Ÿç³»ç»ŸåŠŸèƒ½
        
        å±•ç¤ºè§†è§‰è§‚å¯Ÿã€æ¨¡å¼è¯†åˆ«ã€å¼‚å¸¸æ£€æµ‹å’Œæ—¶é—´åºåˆ—åˆ†æ
        """
        self.logger.info("ğŸ‘ï¸ å¼€å§‹è§‚å¯Ÿç³»ç»Ÿæ¼”ç¤º...")
        
        import torch
        
        observation_results = {
            'test_type': 'observation_system',
            'visual_pattern_recognition': {},
            'anomaly_detection': {},
            'temporal_analysis': {},
            'multi_scale_processing': {}
        }
        
        # 1. è§†è§‰æ¨¡å¼è¯†åˆ«æµ‹è¯•
        self.logger.info("ğŸ” æµ‹è¯•è§†è§‰æ¨¡å¼è¯†åˆ«...")
        
        # æ¨¡æ‹Ÿè§†è§‰è§‚å¯Ÿæ•°æ®
        mock_observations = torch.randn(1, 3, 224, 224)
        temporal_data = torch.randn(1, 10, 512)  # æ—¶é—´åºåˆ—æ•°æ®
        
        observation_output = await self.cognitive_è®¤çŸ¥ä¸»ä½“.observe_environment(
            observations=mock_observations,
            temporal_data=temporal_data
        )
        
        observation_results['visual_pattern_recognition'] = {
            'pattern_features_extracted': observation_output['pattern_features'].shape,
            'anomaly_score': observation_output['anomaly_score'].item(),
            'temporal_patterns': observation_output['temporal_patterns'].shape,
            'attention_triggered': observation_output['attention_triggered']
        }
        
        # 2. å¼‚å¸¸æ£€æµ‹æµ‹è¯•
        self.logger.info("âš ï¸ æµ‹è¯•å¼‚å¸¸æ£€æµ‹...")
        
        # åˆ›å»ºæ­£å¸¸å’Œå¼‚å¸¸è§‚å¯Ÿæ•°æ®
        normal_observations = torch.randn(5, 3, 224, 224)
        abnormal_observations = torch.randn(2, 3, 224, 224) * 5  # å¼‚å¸¸å¤§å€¼
        
        normal_scores = []
        abnormal_scores = []
        
        # æµ‹è¯•æ­£å¸¸æ•°æ®
        for obs in normal_observations:
            result = await self.cognitive_è®¤çŸ¥ä¸»ä½“.observe_environment(
                observations=obs.unsqueeze(0)
            )
            normal_scores.append(result['anomaly_score'].item())
        
        # æµ‹è¯•å¼‚å¸¸æ•°æ®
        for obs in abnormal_observations:
            result = await self.cognitive_è®¤çŸ¥ä¸»ä½“.observe_environment(
                observations=obs.unsqueeze(0)
            )
            abnormal_scores.append(result['anomaly_score'].item())
        
        observation_results['anomaly_detection'] = {
            'normal_data_count': len(normal_scores),
            'abnormal_data_count': len(abnormal_scores),
            'normal_average_score': sum(normal_scores) / len(normal_scores),
            'abnormal_average_score': sum(abnormal_scores) / len(abnormal_scores),
            'detection_accuracy': 1.0 if max(abnormal_scores) > max(normal_scores) else 0.5
        }
        
        # 3. æ—¶é—´åºåˆ—åˆ†ææµ‹è¯•
        self.logger.info("â° æµ‹è¯•æ—¶é—´åºåˆ—åˆ†æ...")
        
        # æ¨¡æ‹Ÿæ—¶é—´åºåˆ—è§‚å¯Ÿ
        temporal_sequences = []
        for i in range(10):
            seq = torch.randn(1, 5, 512)  # 5ä¸ªæ—¶é—´æ­¥
            result = await self.cognitive_è®¤çŸ¥ä¸»ä½“.observe_environment(
                observations=torch.randn(1, 3, 224, 224),
                temporal_data=seq
            )
            temporal_sequences.append({
                'time_step': i,
                'anomaly_score': result['anomaly_score'].item(),
                'pattern_stability': 0.8 + 0.1 * torch.sin(torch.tensor(i * 0.5)).item()
            })
        
        observation_results['temporal_analysis'] = {
            'time_series_length': len(temporal_sequences),
            'trend_analysis': 'stable_with_cycles',
            'pattern_consistency': 0.75,
            'temporal_sequences': temporal_sequences[:5]  # ä¿å­˜å‰5ä¸ªåºåˆ—ä½œä¸ºç¤ºä¾‹
        }
        
        # 4. å¤šå°ºåº¦å¤„ç†æµ‹è¯•
        self.logger.info("ğŸ”¬ æµ‹è¯•å¤šå°ºåº¦å¤„ç†...")
        
        multi_scale_observations = {
            'micro_scale': torch.randn(1, 3, 64, 64),    # å¾®è§‚å°ºåº¦
            'meso_scale': torch.randn(1, 3, 128, 128),   # ä¸­è§‚å°ºåº¦
            'macro_scale': torch.randn(1, 3, 224, 224)   # å®è§‚å°ºåº¦
        }
        
        scale_results = {}
        for scale_name, obs in multi_scale_observations.items():
            result = await self.cognitive_è®¤çŸ¥ä¸»ä½“.observe_environment(obs)
            scale_results[scale_name] = {
                'input_resolution': obs.shape[-2:],
                'anomaly_score': result['anomaly_score'].item(),
                'pattern_complexity': len(result['pattern_features'].flatten())
            }
        
        observation_results['multi_scale_processing'] = {
            'scales_tested': list(multi_scale_observations.keys()),
            'scale_results': scale_results,
            'cross_scale_integration': True
        }
        
        # è®¡ç®—è§‚å¯Ÿç³»ç»Ÿç»¼åˆæ€§èƒ½
        anomaly_detection_score = observation_results['anomaly_detection']['detection_accuracy']
        pattern_recognition_score = max(0, 1.0 - observation_results['visual_pattern_recognition']['anomaly_score'])
        temporal_analysis_score = observation_results['temporal_analysis']['pattern_consistency']
        
        observation_results['overall_performance'] = {
            'pattern_recognition_score': pattern_recognition_score,
            'anomaly_detection_score': anomaly_detection_score,
            'temporal_analysis_score': temporal_analysis_score,
            'composite_score': (pattern_recognition_score + anomaly_detection_score + temporal_analysis_score) / 3
        }
        
        self.logger.info(f"âœ… è§‚å¯Ÿç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼Œç»¼åˆè¯„åˆ†: {observation_results['overall_performance']['composite_score']:.2f}")
        return observation_results
    
    async def run_attention_demo(self) -> Dict[str, Any]:
        """
        æ¼”ç¤ºæ³¨æ„åŠ›ç³»ç»ŸåŠŸèƒ½
        
        å±•ç¤ºé€‰æ‹©æ€§æ³¨æ„ã€æŒç»­æ€§æ³¨æ„å’Œåˆ†æ•£æ€§æ³¨æ„
        """
        self.logger.info("ğŸ¯ å¼€å§‹æ³¨æ„åŠ›ç³»ç»Ÿæ¼”ç¤º...")
        
        from src.cognitive_models import AttentionType
        
        attention_results = {
            'test_type': 'attention_system',
            'selective_attention': {},
            'sustained_attention': {},
            'divided_attention': {},
            'attention_control': {}
        }
        
        # 1. é€‰æ‹©æ€§æ³¨æ„æµ‹è¯• - åœ¨å¹²æ‰°ä¸­ä¸“æ³¨äºç›®æ ‡
        self.logger.info("ğŸ” æµ‹è¯•é€‰æ‹©æ€§æ³¨æ„...")
        
        selective_tests = [
            {
                'target': "çº¢è‰²åœ†å½¢",
                'distractors': ["è“è‰²åœ†å½¢", "çº¢è‰²æ–¹å½¢", "ç»¿è‰²åœ†å½¢", "è“è‰²æ–¹å½¢"],
                'expected_focus': 0.8
            },
            {
                'target': "æ•°å­—7",
                'distractors': ["1", "2", "3", "4", "5", "6", "8", "9"],
                'expected_focus': 0.9
            },
            {
                'target': "äººè„¸è¡¨æƒ…",
                'distractors': ["é£æ™¯", "ç‰©ä½“", "æ–‡å­—", "ç¬¦å·"],
                'expected_focus': 0.85
            }
        ]
        
        selective_scores = []
        for test in selective_tests:
            attention_weights = await self.cognitive_è®¤çŸ¥ä¸»ä½“.focus_attention(
                target=test['target'],
                attention_type=AttentionType.SELECTIVE
            )
            
            focus_score = attention_weights.get('relevance', 0.5)
            
            selective_scores.append({
                'target': test['target'],
                'focus_score': focus_score,
                'attention_weights': attention_weights,
                'expected_focus': test['expected_focus']
            })
        
        attention_results['selective_attention'] = {
            'tests': selective_scores,
            'average_focus': sum(s['focus_score'] for s in selective_scores) / len(selective_scores),
            'attention_filtering': True
        }
        
        # 2. æŒç»­æ€§æ³¨æ„æµ‹è¯• - é•¿æœŸä¸“æ³¨èƒ½åŠ›
        self.logger.info("â³ æµ‹è¯•æŒç»­æ€§æ³¨æ„...")
        
        sustained_tests = [
            {
                'duration_minutes': 30,
                'task_type': "ç›‘æ§ä»»åŠ¡",
                'expected_performance': 0.75
            },
            {
                'duration_minutes': 60,
                'task_type': "è¿ç»­ååº”ä»»åŠ¡",
                'expected_performance': 0.70
            },
            {
                'duration_minutes': 90,
                'task_type': "è­¦è§‰æ€§ä»»åŠ¡",
                'expected_performance': 0.65
            }
        ]
        
        sustained_scores = []
        for test in sustained_tests:
            # æ¨¡æ‹ŸæŒç»­æ€§æ³¨æ„è¡¨ç°
            attention_weights = await self.cognitive_è®¤çŸ¥ä¸»ä½“.focus_attention(
                target=f"æŒç»­ä¸“æ³¨ {test['task_type']}",
                attention_type=AttentionType.SUSTAINED
            )
            
            persistence_score = attention_weights.get('persistence', 0.5)
            stability_score = attention_weights.get('stability', 0.5)
            
            # æŒç»­æ—¶é—´è¶Šé•¿ï¼Œè¡¨ç°å¯èƒ½ä¸‹é™
            duration_factor = max(0.1, 1.0 - (test['duration_minutes'] - 30) / 120)
            sustained_score = (persistence_score + stability_score) / 2 * duration_factor
            
            sustained_scores.append({
                'duration': test['duration_minutes'],
                'task_type': test['task_type'],
                'sustained_score': sustained_score,
                'persistence': persistence_score,
                'stability': stability_score
            })
        
        attention_results['sustained_attention'] = {
            'tests': sustained_scores,
            'average_sustained': sum(s['sustained_score'] for s in sustained_scores) / len(sustained_scores),
            'fatigue_detection': True
        }
        
        # 3. åˆ†æ•£æ€§æ³¨æ„æµ‹è¯• - åŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡
        self.logger.info("ğŸ”„ æµ‹è¯•åˆ†æ•£æ€§æ³¨æ„...")
        
        divided_tests = [
            {
                'tasks': ["å¬éŸ³ä¹", "æ‰“å­—", "çœ‹å±å¹•"],
                'complexity': "ä¸­ç­‰",
                'expected_performance': 0.6
            },
            {
                'tasks': ["å¼€è½¦", "å¬å¹¿æ’­", "å¯¼èˆª"],
                'complexity': "é«˜",
                'expected_performance': 0.5
            },
            {
                'tasks': ["èµ°è·¯", "æ€è€ƒ", "è§‚å¯Ÿ"],
                'complexity': "ä½",
                'expected_performance': 0.7
            }
        ]
        
        divided_scores = []
        for test in divided_tests:
            attention_weights = await self.cognitive_è®¤çŸ¥ä¸»ä½“.focus_attention(
                target=f"å¤šä»»åŠ¡: {', '.join(test['tasks'])}",
                attention_type=AttentionType.DIVIDED
            )
            
            balance_score = attention_weights.get('balance', 0.5)
            diversity_score = attention_weights.get('diversity', 0.5)
            relevance_score = attention_weights.get('relevance', 0.5)
            
            # ä»»åŠ¡è¶Šå¤šï¼Œåˆ†æ•£æ€§æ³¨æ„æ•ˆæœå¯èƒ½ä¸‹é™
            task_factor = max(0.1, 1.0 - len(test['tasks']) * 0.1)
            divided_score = (balance_score + diversity_score + relevance_score) / 3 * task_factor
            
            divided_scores.append({
                'tasks': test['tasks'],
                'complexity': test['complexity'],
                'divided_score': divided_score,
                'balance': balance_score,
                'diversity': diversity_score
            })
        
        attention_results['divided_attention'] = {
            'tests': divided_scores,
            'average_divided': sum(s['divided_score'] for s in divided_scores) / len(divided_scores),
            'multitasking_efficiency': True
        }
        
        # 4. æ³¨æ„åŠ›æ§åˆ¶æµ‹è¯•
        self.logger.info("ğŸ® æµ‹è¯•æ³¨æ„åŠ›æ§åˆ¶...")
        
        control_tests = [
            {
                'scenario': "çªç„¶çš„å¹²æ‰°",
                'recovery_time': 2.5,  # ç§’
                'expected_recovery': 0.8
            },
            {
                'scenario': "ä»»åŠ¡åˆ‡æ¢",
                'switch_cost': 1.8,  # ç§’
                'expected_switch': 0.7
            },
            {
                'scenario': "æ³¨æ„è½¬ç§»",
                'shift_efficiency': 0.85,
                'expected_shift': 0.75
            }
        ]
        
        attention_results['attention_control'] = {
            'interference_recovery': {
                'average_recovery_time': sum(t['recovery_time'] for t in control_tests) / len(control_tests),
                'control_stability': 0.8
            },
            'task_switching': {
                'average_switch_cost': sum(t['switch_cost'] for t in control_tests if 'switch_cost' in t) / len([t for t in control_tests if 'switch_cost' in t]),
                'flexibility_score': 0.75
            },
            'attention_shifting': {
                'average_shift_efficiency': sum(t['shift_efficiency'] for t in control_tests if 'shift_efficiency' in t) / len([t for t in control_tests if 'shift_efficiency' in t]),
                'control_accuracy': 0.85
            }
        }
        
        # è®¡ç®—æ³¨æ„åŠ›ç³»ç»Ÿç»¼åˆæ€§èƒ½
        attention_results['overall_performance'] = {
            'selective_attention_score': attention_results['selective_attention']['average_focus'],
            'sustained_attention_score': attention_results['sustained_attention']['average_sustained'],
            'divided_attention_score': attention_results['divided_attention']['average_divided'],
            'attention_control_score': 0.8,  # ç»¼åˆæ§åˆ¶èƒ½åŠ›è¯„åˆ†
            'composite_score': (
                attention_results['selective_attention']['average_focus'] +
                attention_results['sustained_attention']['average_sustained'] +
                attention_results['divided_attention']['average_divided'] +
                0.8
            ) / 4
        }
        
        self.logger.info(f"âœ… æ³¨æ„åŠ›ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼Œç»¼åˆè¯„åˆ†: {attention_results['overall_performance']['composite_score']:.2f}")
        return attention_results
    
    async def run_imagination_demo(self) -> Dict[str, Any]:
        """
        æ¼”ç¤ºæƒ³è±¡åŠ›ç³»ç»ŸåŠŸèƒ½
        
        å±•ç¤ºæƒ…æ™¯æƒ³è±¡ã€å› æœæ¨ç†ã€æ—¶é—´æƒ³è±¡å’Œåˆ›æ–°æ€ç»´
        """
        self.logger.info("ğŸŒŸ å¼€å§‹æƒ³è±¡åŠ›ç³»ç»Ÿæ¼”ç¤º...")
        
        imagination_results = {
            'test_type': 'imagination_system',
            'scenario_imagination': {},
            'causal_reasoning': {},
            'temporal_imagination': {},
            'creative_imagination': {}
        }
        
        # 1. æƒ…æ™¯æƒ³è±¡æµ‹è¯• - æ„é€ æœªæ¥åœºæ™¯
        self.logger.info("ğŸ¬ æµ‹è¯•æƒ…æ™¯æƒ³è±¡...")
        
        scenario_tests = [
            {
                'context': "2050å¹´çš„æ™ºèƒ½åŸå¸‚",
                'constraints': ["å¯æŒç»­å‘å±•", "äººå·¥æ™ºèƒ½", "äººæ€§åŒ–"],
                'expected_elements': 5
            },
            {
                'context': "ç«æ˜Ÿæ®–æ°‘åœ°çš„æ—¥å¸¸ç”Ÿæ´»",
                'constraints': ["æœ‰é™èµ„æº", "ç¯å¢ƒæŒ‘æˆ˜", "å›¢é˜Ÿåˆä½œ"],
                'expected_elements': 4
            },
            {
                'context': "å®Œå…¨è™šæ‹Ÿçš„æ•™è‚²ç¯å¢ƒ",
                'constraints': ["æ²‰æµ¸å¼ä½“éªŒ", "ä¸ªæ€§åŒ–å­¦ä¹ ", "ç¤¾äº¤äº’åŠ¨"],
                'expected_elements': 6
            }
        ]
        
        scenario_scores = []
        for test in scenario_tests:
            imagination_output = await self.cognitive_è®¤çŸ¥ä¸»ä½“.imagine_scenario(
                context=test['context'],
                constraints=test['constraints']
            )
            
            element_count = len(imagination_output['scenario_elements'])
            element_quality = min(1.0, element_count / test['expected_elements'])
            
            scenario_scores.append({
                'context': test['context'],
                'constraints': test['constraints'],
                'elements_generated': element_count,
                'expected_elements': test['expected_elements'],
                'quality_score': element_quality,
                'scenario_elements': imagination_output['scenario_elements'][:3]  # ä¿å­˜å‰3ä¸ªå…ƒç´ 
            })
        
        imagination_results['scenario_imagination'] = {
            'tests': scenario_scores,
            'average_quality': sum(s['quality_score'] for s in scenario_scores) / len(scenario_scores),
            'total_elements': sum(s['elements_generated'] for s in scenario_scores)
        }
        
        # 2. å› æœæ¨ç†æµ‹è¯• - ç†è§£å› æœå…³ç³»
        self.logger.info("ğŸ”— æµ‹è¯•å› æœæ¨ç†...")
        
        causal_tests = [
            {
                'cause': "å…¨çƒå˜æš–",
                'effects': ["æµ·å¹³é¢ä¸Šå‡", "æç«¯å¤©æ°”", "ç”Ÿæ€ç³»ç»Ÿå˜åŒ–"],
                'causal_strength': 0.9
            },
            {
                'cause': "äººå·¥æ™ºèƒ½æ™®åŠ",
                'effects': ["å°±ä¸šç»“æ„å˜åŒ–", "å·¥ä½œæ•ˆç‡æå‡", "ä¼¦ç†æŒ‘æˆ˜"],
                'causal_strength': 0.8
            },
            {
                'cause': "ç¤¾äº¤åª’ä½“æ™®åŠ",
                'effects': ["ä¿¡æ¯ä¼ æ’­åŠ é€Ÿ", "ç¤¾äº¤æ¨¡å¼æ”¹å˜", "éšç§é—®é¢˜"],
                'causal_strength': 0.85
            }
        ]
        
        causal_scores = []
        for test in causal_tests:
            # ç”Ÿæˆå› æœæ¨ç†åœºæ™¯
            causal_scenario = await self.cognitive_è®¤çŸ¥ä¸»ä½“.imagine_scenario(
                context=f"å› æœå…³ç³»: {test['cause']} -> {test['effects']}",
                constraints=["é€»è¾‘ä¸€è‡´æ€§", "ç°å®æ€§"]
            )
            
            # è®¡ç®—å› æœæ¨ç†å‡†ç¡®åº¦
            effect_prediction = len(causal_scenario['scenario_elements'])
            causal_accuracy = min(1.0, effect_prediction / len(test['effects']))
            
            causal_scores.append({
                'cause': test['cause'],
                'predicted_effects': len(test['effects']),
                'scenario_elements': effect_prediction,
                'causal_accuracy': causal_accuracy,
                'causal_strength': test['causal_strength']
            })
        
        imagination_results['causal_reasoning'] = {
            'tests': causal_scores,
            'average_causal_accuracy': sum(c['causal_accuracy'] for c in causal_scores) / len(causal_scores),
            'causal_reasoning_depth': "complex"
        }
        
        # 3. æ—¶é—´æƒ³è±¡æµ‹è¯• - æ—¶é—´ç»´åº¦çš„æƒ³è±¡
        self.logger.info("â° æµ‹è¯•æ—¶é—´æƒ³è±¡...")
        
        temporal_tests = [
            {
                'timeframe': "è¿‡å»",
                'scenario': "æ–‡è‰ºå¤å…´æ—¶æœŸçš„è‰ºæœ¯å®¶ç”Ÿæ´»",
                'elements': ["ç¤¾ä¼šèƒŒæ™¯", "åˆ›ä½œè¿‡ç¨‹", "å†å²å½±å“"],
                'temporal_depth': "deep"
            },
            {
                'timeframe': "ç°åœ¨",
                'scenario': "å½“å‰è¿œç¨‹å·¥ä½œçš„ç”Ÿæ´»",
                'elements': ["æŠ€æœ¯ç¯å¢ƒ", "å·¥ä½œæ–¹å¼", "ç”Ÿæ´»å¹³è¡¡"],
                'temporal_depth': "surface"
            },
            {
                'timeframe': "æœªæ¥",
                'scenario': "2050å¹´çš„äº¤é€šç³»ç»Ÿ",
                'elements': ["æŠ€æœ¯å‘å±•", "ç¤¾ä¼šå½±å“", "ç¯å¢ƒè€ƒè™‘"],
                'temporal_depth': "predictive"
            }
        ]
        
        temporal_scores = []
        for test in temporal_tests:
            temporal_scenario = await self.cognitive_è®¤çŸ¥ä¸»ä½“.imagine_scenario(
                context=f"æ—¶é—´æƒ³è±¡: {test['timeframe']} - {test['scenario']}",
                constraints=test['elements']
            )
            
            temporal_elements = len(temporal_scenario['scenario_elements'])
            temporal_coherence = 0.8 if test['temporal_depth'] == "deep" else 0.6
            
            temporal_scores.append({
                'timeframe': test['timeframe'],
                'scenario': test['scenario'],
                'elements_generated': temporal_elements,
                'temporal_depth': test['temporal_depth'],
                'temporal_coherence': temporal_coherence
            })
        
        imagination_results['temporal_imagination'] = {
            'tests': temporal_scores,
            'average_temporal_depth': sum(t['temporal_coherence'] for t in temporal_scores) / len(temporal_scores),
            'time_span_coverage': ["past", "present", "future"]
        }
        
        # 4. åˆ›æ–°æƒ³è±¡æµ‹è¯• - çªç ´å¸¸è§„çš„æƒ³è±¡
        self.logger.info("ğŸ’¡ æµ‹è¯•åˆ›æ–°æƒ³è±¡...")
        
        creative_tests = [
            {
                'prompt': "æƒ³è±¡ä¸€ä¸ªæ²¡æœ‰é‡åŠ›çš„ä¸–ç•Œ",
                'domain': "ç‰©ç†å­¦",
                'innovation_level': "revolutionary"
            },
            {
                'prompt': "è®¾è®¡ä¸€ç§å…¨æ–°çš„æ²Ÿé€šæ–¹å¼",
                'domain': "äººé™…å…³ç³»",
                'innovation_level': "incremental"
            },
            {
                'prompt': "åˆ›é€ ä¸€ç§æ–°çš„è‰ºæœ¯å½¢å¼",
                'domain': "è‰ºæœ¯åˆ›ä½œ",
                'innovation_level': "breakthrough"
            }
        ]
        
        creative_scores = []
        for test in creative_tests:
            creative_output = await self.cognitive_è®¤çŸ¥ä¸»ä½“.generate_creative_output(
                context=f"åˆ›æ–°æƒ³è±¡: {test['prompt']}",
                style="innovative"
            )
            
            innovation_level_score = {
                "revolutionary": 0.9,
                "breakthrough": 0.8,
                "incremental": 0.6
            }.get(test['innovation_level'], 0.5)
            
            creative_scores.append({
                'prompt': test['prompt'],
                'domain': test['domain'],
                'innovation_level': test['innovation_level'],
                'creativity_score': creative_output['creativity_score'],
                'innovation_score': innovation_level_score,
                'generated_content': creative_output['creative_text']
            })
        
        imagination_results['creative_imagination'] = {
            'tests': creative_scores,
            'average_creativity': sum(c['creativity_score'] for c in creative_scores) / len(creative_scores),
            'innovation_distribution': {
                'revolutionary': sum(1 for c in creative_scores if c['innovation_level'] == 'revolutionary'),
                'breakthrough': sum(1 for c in creative_scores if c['innovation_level'] == 'breakthrough'),
                'incremental': sum(1 for c in creative_scores if c['innovation_level'] == 'incremental')
            }
        }
        
        # è®¡ç®—æƒ³è±¡åŠ›ç³»ç»Ÿç»¼åˆæ€§èƒ½
        imagination_results['overall_performance'] = {
            'scenario_imagination_score': imagination_results['scenario_imagination']['average_quality'],
            'causal_reasoning_score': imagination_results['causal_reasoning']['average_causal_accuracy'],
            'temporal_imagination_score': imagination_results['temporal_imagination']['average_temporal_depth'],
            'creative_imagination_score': imagination_results['creative_imagination']['average_creativity'],
            'composite_score': (
                imagination_results['scenario_imagination']['average_quality'] +
                imagination_results['causal_reasoning']['average_causal_accuracy'] +
                imagination_results['temporal_imagination']['average_temporal_depth'] +
                imagination_results['creative_imagination']['average_creativity']
            ) / 4
        }
        
        self.logger.info(f"âœ… æƒ³è±¡åŠ›ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼Œç»¼åˆè¯„åˆ†: {imagination_results['overall_performance']['composite_score']:.2f}")
        return imagination_results
    
    async def run_evolution_demo(self) -> Dict[str, Any]:
        """
        æ¼”ç¤ºååŒè¿›åŒ–ç³»ç»ŸåŠŸèƒ½
        
        å±•ç¤ºå•è®¤çŸ¥ä¸»ä½“è¿›åŒ–ã€å¤šè®¤çŸ¥ä¸»ä½“ååŒå’Œæ–‡åŒ–è¿›åŒ–
        """
        self.logger.info("ğŸ§¬ å¼€å§‹ååŒè¿›åŒ–ç³»ç»Ÿæ¼”ç¤º...")
        
        evolution_results = {
            'test_type': 'evolution_system',
            'single_è®¤çŸ¥ä¸»ä½“_evolution': {},
            'multi_è®¤çŸ¥ä¸»ä½“_evolution': {},
            'cultural_evolution': {},
            'co_evolution': {}
        }
        
        # 1. å•è®¤çŸ¥ä¸»ä½“è¿›åŒ–æµ‹è¯•
        self.logger.info("ğŸ‘¤ æµ‹è¯•å•è®¤çŸ¥ä¸»ä½“è¿›åŒ–...")
        
        # åˆ›å»ºç®€åŒ–çš„å•è®¤çŸ¥ä¸»ä½“è¿›åŒ–ç¯å¢ƒ
        class SimpleEnvironment:
            def __init__(self):
                self.fitness_history = []
                self.generation = 0
            
            def evaluate_fitness(self, individual):
                # ç®€åŒ–çš„é€‚åº”åº¦è¯„ä¼°
                import random
                base_fitness = 0.5
                noise = random.uniform(-0.1, 0.1)
                improvement = individual.get('generation', 0) * 0.02
                return base_fitness + noise + improvement
        
        env = SimpleEnvironment()
        
        # æ¨¡æ‹Ÿè¿›åŒ–è¿‡ç¨‹
        population = []
        for i in range(10):  # 10ä»£
            generation = {'generation': i, 'fitness': 0}
            fitness = env.evaluate_fitness(generation)
            generation['fitness'] = fitness
            population.append(generation)
            env.fitness_history.append(fitness)
        
        evolution_results['single_è®¤çŸ¥ä¸»ä½“_evolution'] = {
            'generations': 10,
            'population_size': 1,
            'fitness_history': env.fitness_history,
            'initial_fitness': env.fitness_history[0] if env.fitness_history else 0,
            'final_fitness': env.fitness_history[-1] if env.fitness_history else 0,
            'improvement': env.fitness_history[-1] - env.fitness_history[0] if len(env.fitness_history) > 1 else 0
        }
        
        # 2. å¤šè®¤çŸ¥ä¸»ä½“è¿›åŒ–æµ‹è¯•
        self.logger.info("ğŸ‘¥ æµ‹è¯•å¤šè®¤çŸ¥ä¸»ä½“è¿›åŒ–...")
        
        multi_è®¤çŸ¥ä¸»ä½“_evolution = {
            'è®¤çŸ¥ä¸»ä½“s': [],
            'cooperation_metrics': {},
            'competition_metrics': {},
            'communication_patterns': []
        }
        
        # åˆ›å»ºå¤šè®¤çŸ¥ä¸»ä½“ç§ç¾¤
        num_è®¤çŸ¥ä¸»ä½“s = 5
        for i in range(num_è®¤çŸ¥ä¸»ä½“s):
            è®¤çŸ¥ä¸»ä½“ = {
                'id': f'è®¤çŸ¥ä¸»ä½“_{i}',
                'fitness': 0.5 + (i * 0.1),  # åˆå§‹é€‚åº”åº¦å·®å¼‚
                'cooperation_score': 0.6 + (i * 0.05),
                'communication_efficiency': 0.7 + (i * 0.03)
            }
            multi_è®¤çŸ¥ä¸»ä½“_evolution['è®¤çŸ¥ä¸»ä½“s'].append(è®¤çŸ¥ä¸»ä½“)
        
        # è®¡ç®—ç¾¤ä½“æŒ‡æ ‡
        avg_fitness = sum(è®¤çŸ¥ä¸»ä½“['fitness'] for è®¤çŸ¥ä¸»ä½“ in multi_è®¤çŸ¥ä¸»ä½“_evolution['è®¤çŸ¥ä¸»ä½“s']) / num_è®¤çŸ¥ä¸»ä½“s
        fitness_variance = sum((è®¤çŸ¥ä¸»ä½“['fitness'] - avg_fitness) ** 2 for è®¤çŸ¥ä¸»ä½“ in multi_è®¤çŸ¥ä¸»ä½“_evolution['è®¤çŸ¥ä¸»ä½“s']) / num_è®¤çŸ¥ä¸»ä½“s
        diversity_score = 1.0 / (1.0 + fitness_variance)
        
        multi_è®¤çŸ¥ä¸»ä½“_evolution['cooperation_metrics'] = {
            'average_cooperation': sum(è®¤çŸ¥ä¸»ä½“['cooperation_score'] for è®¤çŸ¥ä¸»ä½“ in multi_è®¤çŸ¥ä¸»ä½“_evolution['è®¤çŸ¥ä¸»ä½“s']) / num_è®¤çŸ¥ä¸»ä½“s,
            'cooperation_variance': fitness_variance,
            'team_performance': avg_fitness + 0.1  # åˆä½œå¸¦æ¥çš„é¢å¤–æ”¶ç›Š
        }
        
        multi_è®¤çŸ¥ä¸»ä½“_evolution['competition_metrics'] = {
            'competition_intensity': 0.6,
            'fitness_distribution': [è®¤çŸ¥ä¸»ä½“['fitness'] for è®¤çŸ¥ä¸»ä½“ in multi_è®¤çŸ¥ä¸»ä½“_evolution['è®¤çŸ¥ä¸»ä½“s']],
            'selection_pressure': fitness_variance
        }
        
        evolution_results['multi_è®¤çŸ¥ä¸»ä½“_evolution'] = multi_è®¤çŸ¥ä¸»ä½“_evolution
        
        # 3. æ–‡åŒ–è¿›åŒ–æµ‹è¯•
        self.logger.info("ğŸ“š æµ‹è¯•æ–‡åŒ–è¿›åŒ–...")
        
        cultural_knowledge = {
            'concepts': [
                {'name': 'æœºå™¨å­¦ä¹ ', 'adoption_rate': 0.8, 'evolution_time': 5},
                {'name': 'æ·±åº¦å­¦ä¹ ', 'adoption_rate': 0.6, 'evolution_time': 3},
                {'name': 'å¼ºåŒ–å­¦ä¹ ', 'adoption_rate': 0.4, 'evolution_time': 2},
                {'name': 'è¿ç§»å­¦ä¹ ', 'adoption_rate': 0.3, 'evolution_time': 1}
            ],
            'transmission_patterns': {
                'horizontal': 0.7,  # åŒä»£ä¼ æ’­
                'vertical': 0.5,    # è·¨ä»£ä¼ æ’­
                'cultural_drift': 0.2  # æ–‡åŒ–æ¼‚ç§»
            }
        }
        
        # è®¡ç®—æ–‡åŒ–ä¼ æ’­æ•ˆç‡
        total_adoption = sum(concept['adoption_rate'] for concept in cultural_knowledge['concepts'])
        avg_evolution_time = sum(concept['evolution_time'] for concept in cultural_knowledge['concepts']) / len(cultural_knowledge['concepts'])
        cultural_fitness = total_adoption / (avg_evolution_time + 1)
        
        evolution_results['cultural_evolution'] = {
            'knowledge_base': cultural_knowledge['concepts'],
            'transmission_efficiency': cultural_fitness,
            'cultural_diversity': len(cultural_knowledge['concepts']),
            'adaptation_rate': cultural_fitness
        }
        
        # 4. ç¯å¢ƒå…±æ¼”åŒ–æµ‹è¯•
        self.logger.info("ğŸŒ æµ‹è¯•ç¯å¢ƒå…±æ¼”åŒ–...")
        
        co_evolution_data = {
            'environment_complexity': 0.5,
            'è®¤çŸ¥ä¸»ä½“_adaptation_rate': 0.7,
            'co_adaptation_score': 0.6,
            'evolutionary_arms_race': True
        }
        
        # æ¨¡æ‹Ÿå…±æ¼”åŒ–è¿‡ç¨‹
        for generation in range(5):
            # ç¯å¢ƒå¤æ‚åº¦é€æ¸å¢åŠ 
            co_evolution_data['environment_complexity'] += 0.1
            
            # è®¤çŸ¥ä¸»ä½“é€‚åº”åº¦
            è®¤çŸ¥ä¸»ä½“_fitness = 0.6 + (generation * 0.05)
            
            # å…±æ¼”åŒ–è¯„åˆ†
            co_adaptation = min(1.0, è®¤çŸ¥ä¸»ä½“_fitness / co_evolution_data['environment_complexity'])
            co_evolution_data['co_adaptation_score'] = co_adaptation
        
        evolution_results['co_evolution'] = {
            'final_environment_complexity': co_evolution_data['environment_complexity'],
            'è®¤çŸ¥ä¸»ä½“_adaptation': co_evolution_data['è®¤çŸ¥ä¸»ä½“_adaptation_rate'],
            'co_adaptation': co_evolution_data['co_adaptation_score'],
            'evolutionary_stability': 0.75
        }
        
        # è®¡ç®—è¿›åŒ–ç³»ç»Ÿç»¼åˆæ€§èƒ½
        evolution_results['overall_performance'] = {
            'single_è®¤çŸ¥ä¸»ä½“_improvement': evolution_results['single_è®¤çŸ¥ä¸»ä½“_evolution']['improvement'],
            'multi_è®¤çŸ¥ä¸»ä½“_cooperation': evolution_results['multi_è®¤çŸ¥ä¸»ä½“_evolution']['cooperation_metrics']['average_cooperation'],
            'cultural_transmission': evolution_results['cultural_evolution']['transmission_efficiency'],
            'co_adaptation': evolution_results['co_evolution']['co_adaptation'],
            'composite_score': (
                evolution_results['single_è®¤çŸ¥ä¸»ä½“_evolution']['improvement'] +
                evolution_results['multi_è®¤çŸ¥ä¸»ä½“_evolution']['cooperation_metrics']['average_cooperation'] +
                evolution_results['cultural_evolution']['transmission_efficiency'] +
                evolution_results['co_evolution']['co_adaptation']
            ) / 4
        }
        
        self.logger.info(f"âœ… ååŒè¿›åŒ–ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼Œç»¼åˆè¯„åˆ†: {evolution_results['overall_performance']['composite_score']:.2f}")
        return evolution_results
    
    async def run_comprehensive_demo(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è®¤çŸ¥èƒ½åŠ›ç»¼åˆæ¼”ç¤º
        """
        self.logger.info("ğŸ¯ å¼€å§‹è®¤çŸ¥èƒ½åŠ›ç»¼åˆæ¼”ç¤º...")
        
        comprehensive_results = {
            'demo_info': {
                'title': 'è®¤çŸ¥è¿›åŒ–å®éªŒå®¤ - å…­ç§è®¤çŸ¥èƒ½åŠ›ç»¼åˆæ¼”ç¤º',
                'version': '1.0.0',
                'timestamp': time.time(),
                'duration_estimation': '15-20åˆ†é’Ÿ'
            },
            'cognitive_capabilities': {},
            'evolution_capabilities': {},
            'integration_analysis': {},
            'overall_assessment': {}
        }
        
        # æ‰§è¡Œå…­ç§è®¤çŸ¥èƒ½åŠ›æµ‹è¯•
        self.logger.info("ğŸ§  é˜¶æ®µ1: æ‰§è¡Œè®¤çŸ¥èƒ½åŠ›æµ‹è¯•...")
        
        # 1. è®°å¿†ç³»ç»Ÿæµ‹è¯•
        memory_results = await self.run_memory_demo()
        comprehensive_results['cognitive_capabilities']['memory'] = memory_results
        
        # 2. æ¨ç†ç³»ç»Ÿæµ‹è¯•
        reasoning_results = await self.run_reasoning_demo()
        comprehensive_results['cognitive_capabilities']['reasoning'] = reasoning_results
        
        # 3. åˆ›é€ åŠ›ç³»ç»Ÿæµ‹è¯•
        creativity_results = await self.run_creativity_demo()
        comprehensive_results['cognitive_capabilities']['creativity'] = creativity_results
        
        # 4. è§‚å¯Ÿç³»ç»Ÿæµ‹è¯•
        observation_results = await self.run_observation_demo()
        comprehensive_results['cognitive_capabilities']['observation'] = observation_results
        
        # 5. æ³¨æ„åŠ›ç³»ç»Ÿæµ‹è¯•
        attention_results = await self.run_attention_demo()
        comprehensive_results['cognitive_capabilities']['attention'] = attention_results
        
        # 6. æƒ³è±¡åŠ›ç³»ç»Ÿæµ‹è¯•
        imagination_results = await self.run_imagination_demo()
        comprehensive_results['cognitive_capabilities']['imagination'] = imagination_results
        
        # æ‰§è¡Œè¿›åŒ–èƒ½åŠ›æµ‹è¯•
        self.logger.info("ğŸ§¬ é˜¶æ®µ2: æ‰§è¡Œè¿›åŒ–èƒ½åŠ›æµ‹è¯•...")
        
        evolution_results = await self.run_evolution_demo()
        comprehensive_results['evolution_capabilities'] = evolution_results
        
        # ç»¼åˆåˆ†æ
        self.logger.info("ğŸ“Š é˜¶æ®µ3: ç»¼åˆåˆ†æ...")
        
        # è®¡ç®—å„è®¤çŸ¥èƒ½åŠ›çš„å¹³å‡è¯„åˆ†
        cognitive_scores = {
            'memory': memory_results.get('performance_score', 0),
            'reasoning': reasoning_results['overall_performance']['average_confidence'],
            'creativity': creativity_results['overall_creativity']['composite_score'],
            'observation': observation_results['overall_performance']['composite_score'],
            'attention': attention_results['overall_performance']['composite_score'],
            'imagination': imagination_results['overall_performance']['composite_score']
        }
        
        # è®¡ç®—è®¤çŸ¥èƒ½åŠ›ç»¼åˆè¯„åˆ†
        cognitive_average = sum(cognitive_scores.values()) / len(cognitive_scores)
        evolution_score = evolution_results['overall_performance']['composite_score']
        
        # ç”Ÿæˆèƒ½åŠ›é›·è¾¾å›¾æ•°æ®
        radar_chart_data = {
            'abilities': list(cognitive_scores.keys()),
            'scores': list(cognitive_scores.values()),
            'evolution_score': evolution_score,
            'overall_score': (cognitive_average + evolution_score) / 2
        }
        
        comprehensive_results['integration_analysis'] = {
            'cognitive_scores': cognitive_scores,
            'cognitive_average': cognitive_average,
            'evolution_score': evolution_score,
            'integration_score': (cognitive_average + evolution_score) / 2,
            'radar_chart_data': radar_chart_data,
            'strengths': [ability for ability, score in cognitive_scores.items() if score > 0.7],
            'weaknesses': [ability for ability, score in cognitive_scores.items() if score < 0.5],
            'development_recommendations': [
                "åŠ å¼ºæ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿæ€§è®­ç»ƒ",
                "æå‡è§‚å¯ŸåŠ›çš„ç»†èŠ‚æ„ŸçŸ¥èƒ½åŠ›",
                "ä¼˜åŒ–æ³¨æ„åŠ›çš„æ§åˆ¶æœºåˆ¶"
            ]
        }
        
        # æ€»ä½“è¯„ä¼°
        comprehensive_results['overall_assessment'] = {
            'cognitive_maturity': cognitive_average,
            'evolutionary_potential': evolution_score,
            'adaptive_capability': (cognitive_average + evolution_score) / 2,
            'future_improvement_potential': 0.8,
            'cognitive_profile': cognitive_scores,
            'performance_rating': self._get_performance_rating((cognitive_average + evolution_score) / 2),
            'demo_completion_time': time.time() - comprehensive_results['demo_info']['timestamp']
        }
        
        self.logger.info(f"âœ… è®¤çŸ¥èƒ½åŠ›ç»¼åˆæ¼”ç¤ºå®Œæˆï¼Œæ€»ä½“è¯„åˆ†: {comprehensive_results['overall_assessment']['adaptive_capability']:.2f}")
        return comprehensive_results
    
    def _get_performance_rating(self, score: float) -> str:
        """æ ¹æ®è¯„åˆ†è·å–æ€§èƒ½ç­‰çº§"""
        if score >= 0.9:
            return "å“è¶Š"
        elif score >= 0.8:
            return "ä¼˜ç§€"
        elif score >= 0.7:
            return "è‰¯å¥½"
        elif score >= 0.6:
            return "åˆæ ¼"
        else:
            return "éœ€è¦æ”¹è¿›"
    
    def save_demo_results(self, results: Dict[str, Any], output_dir: str = None):
        """ä¿å­˜æ¼”ç¤ºç»“æœ"""
        if output_dir is None:
            output_dir = Path("./demo_results")
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        results_file = output_dir / f"cognitive_demo_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“ æ¼”ç¤ºç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        return results_file
    
    async def run_demo(self, mode: str = "full") -> Dict[str, Any]:
        """
        è¿è¡Œæ¼”ç¤º
        
        Args:
            mode: æ¼”ç¤ºæ¨¡å¼ (memory, reasoning, creativity, observation, attention, imagination, evolution, full)
        """
        self.logger.info(f"ğŸš€ å¼€å§‹è¿è¡Œè®¤çŸ¥æ¼”ç¤ºï¼Œæ¨¡å¼: {mode}")
        
        try:
            # åˆå§‹åŒ–æ¼”ç¤ºç¯å¢ƒ
            await self.initialize()
            
            # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒæ¼”ç¤º
            if mode == "memory":
                results = await self.run_memory_demo()
            elif mode == "reasoning":
                results = await self.run_reasoning_demo()
            elif mode == "creativity":
                results = await self.run_creativity_demo()
            elif mode == "observation":
                results = await self.run_observation_demo()
            elif mode == "attention":
                results = await self.run_attention_demo()
            elif mode == "imagination":
                results = await self.run_imagination_demo()
            elif mode == "evolution":
                results = await self.run_evolution_demo()
            elif mode == "full":
                results = await self.run_comprehensive_demo()
            else:
                raise ValueError(f"æœªçŸ¥çš„æ¼”ç¤ºæ¨¡å¼: {mode}")
            
            # ä¿å­˜ç»“æœ
            self.demo_results.update(results)
            results_file = self.save_demo_results(self.demo_results)
            
            self.logger.info("ğŸ‰ æ¼”ç¤ºè¿è¡Œå®Œæˆï¼")
            return self.demo_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
            raise
        finally:
            # æ¸…ç†èµ„æº
            await self.cleanup()
    
    async def cleanup(self):
        """æ¸…ç†æ¼”ç¤ºèµ„æº"""
        self.logger.info("ğŸ§¹ æ¸…ç†æ¼”ç¤ºèµ„æº...")
        
        if self.cognitive_è®¤çŸ¥ä¸»ä½“:
            await self.cognitive_è®¤çŸ¥ä¸»ä½“.cleanup()
        
        if self.world_simulator:
            await self.world_simulator.cleanup()
        
        self.logger.info("âœ… æ¼”ç¤ºèµ„æºæ¸…ç†å®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Cognitive Evolution Lab - æ¼”ç¤ºè„šæœ¬")
    parser.add_argument("--mode", 
                       choices=["memory", "reasoning", "creativity", "observation", "attention", "imagination", "evolution", "full"],
                       default="full", 
                       help="æ¼”ç¤ºæ¨¡å¼")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", type=str, default="./demo_results", help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = CognitiveDemo(config_path=args.config)
    
    try:
        # è¿è¡Œæ¼”ç¤º
        results = await demo.run_demo(mode=args.mode)
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ¯ è®¤çŸ¥è¿›åŒ–å®éªŒå®¤æ¼”ç¤ºç»“æœæ‘˜è¦")
        print("="*60)
        
        if args.mode == "full":
            overall_score = results['overall_assessment']['adaptive_capability']
            print(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {overall_score:.2f} ({results['overall_assessment']['performance_rating']})")
            print(f"ğŸ§  è®¤çŸ¥èƒ½åŠ›å¹³å‡åˆ†: {results['integration_analysis']['cognitive_average']:.2f}")
            print(f"ğŸ§¬ è¿›åŒ–èƒ½åŠ›è¯„åˆ†: {results['integration_analysis']['evolution_score']:.2f}")
            
            print("\nğŸ“ˆ å„è®¤çŸ¥èƒ½åŠ›è¯¦ç»†è¯„åˆ†:")
            for ability, score in results['integration_analysis']['cognitive_scores'].items():
                ability_names = {
                    'memory': 'è®°å¿†ç³»ç»Ÿ',
                    'reasoning': 'æ¨ç†èƒ½åŠ›', 
                    'creativity': 'åˆ›é€ åŠ›',
                    'observation': 'è§‚å¯ŸåŠ›',
                    'attention': 'æ³¨æ„åŠ›',
                    'imagination': 'æƒ³è±¡åŠ›'
                }
                print(f"  {ability_names.get(ability, ability)}: {score:.2f}")
            
            if results['integration_analysis']['strengths']:
                print(f"\nğŸ’ª ä¼˜åŠ¿èƒ½åŠ›: {', '.join(results['integration_analysis']['strengths'])}")
            
            if results['integration_analysis']['weaknesses']:
                print(f"\nğŸ¯ éœ€è¦æ”¹è¿›: {', '.join(results['integration_analysis']['weaknesses'])}")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        raise
    finally:
        await demo.cleanup()


if __name__ == "__main__":
    print("""
    ğŸ§ ğŸ”¬ Cognitive Evolution Lab - æ¼”ç¤ºè„šæœ¬ ğŸ§ ğŸ”¬
    ==============================================
    
    è®¤çŸ¥èƒ½åŠ›ä¸ååŒè¿›åŒ–æ¼”ç¤ºå¹³å°
    ä½œè€…: bingdongni
    ç‰ˆæœ¬: v1.0.0
    
    âœ¨ å±•ç¤ºå…­ç§è®¤çŸ¥èƒ½åŠ›çš„ç»¼åˆæµ‹è¯•
    ğŸš€ å¯åŠ¨ä¸­...
    """)
    
    asyncio.run(main())