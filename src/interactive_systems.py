#!/usr/bin/env python3
"""
Cognitive Evolution Lab - äº¤äº’è¡ŒåŠ¨ç³»ç»Ÿ
ä½œè€…: bingdongni

å®ç°äº¤äº’è¡ŒåŠ¨ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š
- å…·èº«æ™ºèƒ½ï¼ˆè¿åŠ¨æ§åˆ¶ã€æ„Ÿè§‰èåˆã€å¹³è¡¡æ§åˆ¶ï¼‰
- å¤šæ¨¡æ€æ„ŸçŸ¥ï¼ˆè§†è§‰ã€å¬è§‰ã€è§¦è§‰ã€æ–‡æœ¬ï¼‰
- åŠ¨ä½œè§„åˆ’ï¼ˆç­–ç•¥ç”Ÿæˆã€æ‰§è¡Œæ§åˆ¶ã€å®‰å…¨çº¦æŸï¼‰
- ç¯å¢ƒäº¤äº’ï¼ˆç‰©ç†æ¥è§¦ã€ç¤¾äº¤äº’åŠ¨ã€ä»»åŠ¡æ‰§è¡Œï¼‰
"""

import asyncio
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Any, Optional, Tuple, Union
import logging
from pathlib import Path
import json
import random
from dataclasses import dataclass, field
from enum import Enum
import math
from collections import deque, defaultdict

# å°è¯•å¯¼å…¥ç›¸å…³åº“
try:
    import cv2
    OPENCV_AVè®¤çŸ¥è®¡ç®—LABLE = True
except ImportError:
    OPENCV_AVè®¤çŸ¥è®¡ç®—LABLE = False

try:
    import pygame
    PYGAME_AVè®¤çŸ¥è®¡ç®—LABLE = True
except ImportError:
    PYGAME_AVè®¤çŸ¥è®¡ç®—LABLE = False


class ActionType(Enum):
    """åŠ¨ä½œç±»å‹æšä¸¾"""
    MOTOR_ACTION = "motor_action"      # ç”µæœºåŠ¨ä½œ
    MANIPULATION = "manipulation"      # æ“ä½œåŠ¨ä½œ
    COMMUNICATION = "communication"    # äº¤æµåŠ¨ä½œ
    COGNITIVE_ACTION = "cognitive_action"  # è®¤çŸ¥åŠ¨ä½œ
    SOCIAL_ACTION = "social_action"    # ç¤¾äº¤åŠ¨ä½œ


class SensorType(Enum):
    """ä¼ æ„Ÿå™¨ç±»å‹æšä¸¾"""
    VISION = "vision"
    AUDIO = "audio"
    TOUCH = "touch"
    PROPRIOCEPTION = "proprioception"
    TEXT = "text"


@dataclass
class SensorReading:
    """ä¼ æ„Ÿå™¨è¯»æ•°"""
    sensor_type: SensorType
    data: Any
    timestamp: float
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ActionCommand:
    """åŠ¨ä½œå‘½ä»¤"""
    action_type: ActionType
    parameters: Dict[str, Any]
    duration: float
    priority: int
    safety_constraints: List[str] = field(default_factory=list)


@dataclass
class BodyState:
    """èº«ä½“çŠ¶æ€"""
    position: np.ndarray
    velocity: np.ndarray
    orientation: np.ndarray
    joint_angles: np.ndarray
    joint_velocities: np.ndarray
    force_sensors: Dict[str, float]
    balance_metrics: Dict[str, float]


@dataclass
class InteractionEvent:
    """äº¤äº’äº‹ä»¶"""
    event_type: str
    participants: List[str]
    intensity: float
    timestamp: float
    outcome: Dict[str, Any] = field(default_factory=dict)


class MultimodalPerception:
    """å¤šæ¨¡æ€æ„ŸçŸ¥æ¨¡å—"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # å„æ¨¡æ€é…ç½®
        self.vision_config = config.get('vision', {})
        self.audio_config = config.get('audio', {})
        self.touch_config = config.get('touch', {})
        self.text_config = config.get('text', {})
        
        # åˆå§‹åŒ–å„æ¨¡æ€å¤„ç†å™¨
        self.vision_processor = None
        self.audio_processor = None
        self.touch_processor = None
        self.text_processor = None
        
        # ä¼ æ„Ÿå™¨æ•°æ®ç¼“å†²
        self.sensor_buffers = {
            SensorType.VISION: deque(maxlen=100),
            SensorType.AUDIO: deque(maxlen=50),
            SensorType.TOUCH: deque(maxlen=20),
            SensorType.PROPRIOCEPTION: deque(maxlen=30),
            SensorType.TEXT: deque(maxlen=10)
        }
        
        # æ„ŸçŸ¥èåˆå™¨
        self.perception_fusion = None
        
        self.logger.info("ğŸ” å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """åˆå§‹åŒ–å„æ¨¡æ€å¤„ç†å™¨"""
        self.logger.info("ğŸ”§ åˆå§‹åŒ–å¤šæ¨¡æ€å¤„ç†å™¨...")
        
        try:
            # åˆå§‹åŒ–è§†è§‰å¤„ç†
            await self._initialize_vision_processing()
            
            # åˆå§‹åŒ–å¬è§‰å¤„ç†
            await self._initialize_audio_processing()
            
            # åˆå§‹åŒ–è§¦è§‰å¤„ç†
            await self._initialize_touch_processing()
            
            # åˆå§‹åŒ–æ–‡æœ¬å¤„ç†
            await self._initialize_text_processing()
            
            # åˆå§‹åŒ–æ„ŸçŸ¥èåˆ
            await self._initialize_perception_fusion()
            
            self.logger.info("âœ… å¤šæ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ å¤šæ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _initialize_vision_processing(self):
        """åˆå§‹åŒ–è§†è§‰å¤„ç†"""
        if not OPENCV_AVè®¤çŸ¥è®¡ç®—LABLE:
            self.logger.warning("âš ï¸ OpenCVä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–è§†è§‰å¤„ç†")
            self.vision_processor = self._simple_vision_processor
            return
        
        # åˆå§‹åŒ–è§†è§‰æ¨¡å‹ï¼ˆè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ¨¡å‹ï¼‰
        self.vision_processor = self._opencv_vision_processor
        
        self.logger.info("âœ… è§†è§‰å¤„ç†åˆå§‹åŒ–å®Œæˆ")
    
    async def _initialize_audio_processing(self):
        """åˆå§‹åŒ–å¬è§‰å¤„ç†"""
        # ç®€åŒ–çš„éŸ³é¢‘å¤„ç†
        self.audio_processor = self._simple_audio_processor
        self.logger.info("âœ… å¬è§‰å¤„ç†åˆå§‹åŒ–å®Œæˆ")
    
    async def _initialize_touch_processing(self):
        """åˆå§‹åŒ–è§¦è§‰å¤„ç†"""
        # ç®€åŒ–çš„è§¦è§‰å¤„ç†
        self.touch_processor = self._simple_touch_processor
        self.logger.info("âœ… è§¦è§‰å¤„ç†åˆå§‹åŒ–å®Œæˆ")
    
    async def _initialize_text_processing(self):
        """åˆå§‹åŒ–æ–‡æœ¬å¤„ç†"""
        # ç®€åŒ–çš„æ–‡æœ¬å¤„ç†
        self.text_processor = self._simple_text_processor
        self.logger.info("âœ… æ–‡æœ¬å¤„ç†åˆå§‹åŒ–å®Œæˆ")
    
    async def _initialize_perception_fusion(self):
        """åˆå§‹åŒ–æ„ŸçŸ¥èåˆ"""
        # ç®€åŒ–çš„æ„ŸçŸ¥èåˆ
        self.perception_fusion = self._simple_fusion
        self.logger.info("âœ… æ„ŸçŸ¥èåˆåˆå§‹åŒ–å®Œæˆ")
    
    async def capture_vision(self) -> SensorReading:
        """æ•è·è§†è§‰ä¿¡æ¯"""
        try:
            vision_data = await self.vision_processor()
            
            reading = SensorReading(
                sensor_type=SensorType.VISION,
                data=vision_data,
                timestamp=self._get_timestamp(),
                confidence=0.9,
                metadata={'resolution': '640x480', 'fps': 30}
            )
            
            self.sensor_buffers[SensorType.VISION].append(reading)
            return reading
            
        except Exception as e:
            self.logger.error(f"è§†è§‰æ•è·å¤±è´¥: {e}")
            return self._create_fallback_reading(SensorType.VISION)
    
    async def capture_audio(self) -> SensorReading:
        """æ•è·å¬è§‰ä¿¡æ¯"""
        try:
            audio_data = await self.audio_processor()
            
            reading = SensorReading(
                sensor_type=SensorType.AUDIO,
                data=audio_data,
                timestamp=self._get_timestamp(),
                confidence=0.8,
                metadata={'sample_rate': 16000, 'channels': 1}
            )
            
            self.sensor_buffers[SensorType.AUDIO].append(reading)
            return reading
            
        except Exception as e:
            self.logger.error(f"éŸ³é¢‘æ•è·å¤±è´¥: {e}")
            return self._create_fallback_reading(SensorType.AUDIO)
    
    async def capture_touch(self) -> SensorReading:
        """æ•è·è§¦è§‰ä¿¡æ¯"""
        try:
            touch_data = await self.touch_processor()
            
            reading = SensorReading(
                sensor_type=SensorType.TOUCH,
                data=touch_data,
                timestamp=self._get_timestamp(),
                confidence=0.85,
                metadata={'sensor_count': 100, 'sensitivity': 0.8}
            )
            
            self.sensor_buffers[SensorType.TOUCH].append(reading)
            return reading
            
        except Exception as e:
            self.logger.error(f"è§¦è§‰æ•è·å¤±è´¥: {e}")
            return self._create_fallback_reading(SensorType.TOUCH)
    
    async def capture_proprioception(self, body_state: BodyState) -> SensorReading:
        """æ•è·æœ¬ä½“æ„Ÿå—"""
        proprioceptive_data = {
            'position': body_state.position,
            'velocity': body_state.velocity,
            'joint_angles': body_state.joint_angles,
            'joint_velocities': body_state.joint_velocities,
            'balance_metrics': body_state.balance_metrics
        }
        
        reading = SensorReading(
            sensor_type=SensorType.PROPRIOCEPTION,
            data=proprioceptive_data,
            timestamp=self._get_timestamp(),
            confidence=0.95,
            metadata={'joint_count': len(body_state.joint_angles)}
        )
        
        self.sensor_buffers[SensorType.PROPRIOCEPTION].append(reading)
        return reading
    
    async def process_text_input(self, text: str) -> SensorReading:
        """å¤„ç†æ–‡æœ¬è¾“å…¥"""
        text_data = await self.text_processor(text)
        
        reading = SensorReading(
            sensor_type=SensorType.TEXT,
            data=text_data,
            timestamp=self._get_timestamp(),
            confidence=0.9,
            metadata={'input_length': len(text)}
        )
        
        self.sensor_buffers[SensorType.TEXT].append(reading)
        return reading
    
    async def _opencv_vision_processor(self) -> Dict[str, Any]:
        """OpenCVè§†è§‰å¤„ç†å™¨"""
        if not OPENCV_AVè®¤çŸ¥è®¡ç®—LABLE:
            return self._simple_vision_processor()
        
        try:
            # æ¨¡æ‹Ÿç›¸æœºè¾“å…¥ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä»çœŸå®ç›¸æœºè·å–ï¼‰
            frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            
            # ç®€åŒ–çš„ç‰¹å¾æå–
            features = {
                'objects': self._detect_objects_simple(frame),
                'depth': np.random.uniform(0.5, 10.0, (480, 640)),
                'motion': np.random.random((480, 640, 2)) * 0.1,
                'brightness': float(np.mean(frame)),
                'contrast': float(np.std(frame))
            }
            
            return {
                'frame': frame,
                'features': features,
                'timestamp': self._get_timestamp()
            }
            
        except Exception as e:
            self.logger.error(f"OpenCVè§†è§‰å¤„ç†å¤±è´¥: {e}")
            return self._simple_vision_processor()
    
    def _simple_vision_processor(self) -> Dict[str, Any]:
        """ç®€åŒ–è§†è§‰å¤„ç†å™¨"""
        return {
            'simulated_view': True,
            'objects': ['object1', 'object2', 'object3'],
            'depth_map': np.random.uniform(0.5, 10.0, (100, 100)),
            'brightness': 128.0,
            'motion_vectors': np.random.random((10, 10, 2)),
            'timestamp': self._get_timestamp()
        }
    
    async def _simple_audio_processor(self) -> Dict[str, Any]:
        """ç®€åŒ–éŸ³é¢‘å¤„ç†å™¨"""
        # æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
        sample_rate = 16000
        duration = 1.0
        samples = int(sample_rate * duration)
        
        audio_data = np.random.normal(0, 0.1, samples)
        
        features = {
            'mfcc': np.random.random((13, 100)),
            'spectral_centroid': np.random.uniform(1000, 4000),
            'rms_energy': float(np.sqrt(np.mean(audio_data**2))),
            'zero_crossing_rate': np.random.uniform(0.1, 0.3)
        }
        
        return {
            'audio_data': audio_data,
            'features': features,
            'sample_rate': sample_rate,
            'duration': duration,
            'timestamp': self._get_timestamp()
        }
    
    async def _simple_touch_processor(self) -> Dict[str, Any]:
        """ç®€åŒ–è§¦è§‰å¤„ç†å™¨"""
        sensor_count = 100
        touch_data = {
            'pressure': np.random.uniform(0, 1, sensor_count),
            'temperature': np.random.uniform(20, 40, sensor_count),
            'vibration': np.random.uniform(0, 0.1, sensor_count),
            'texture': np.random.uniform(0, 1, sensor_count)
        }
        
        return {
            'touch_data': touch_data,
            'sensor_count': sensor_count,
            'timestamp': self._get_timestamp()
        }
    
    async def _simple_text_processor(self, text: str) -> Dict[str, Any]:
        """ç®€åŒ–æ–‡æœ¬å¤„ç†å™¨"""
        words = text.lower().split()
        
        features = {
            'word_count': len(words),
            'sentiment_score': np.random.uniform(-1, 1),
            'keywords': words[:5],  # å‰5ä¸ªè¯ä½œä¸ºå…³é”®è¯
            'language': 'chinese' if any(ord(char) > 127 for char in text) else 'english',
            'complexity': len(set(words)) / len(words) if words else 0
        }
        
        return {
            'text': text,
            'features': features,
            'timestamp': self._get_timestamp()
        }
    
    def _detect_objects_simple(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """ç®€åŒ–ç‰©ä½“æ£€æµ‹"""
        # æ¨¡æ‹Ÿç‰©ä½“æ£€æµ‹ç»“æœ
        object_count = random.randint(3, 8)
        objects = []
        
        for i in range(object_count):
            obj = {
                'class': random.choice(['person', 'car', 'dog', 'cat', 'chair', 'table']),
                'bbox': [random.randint(0, 100) for _ in range(4)],
                'confidence': random.uniform(0.6, 0.95),
                'center': (random.randint(0, 640), random.randint(0, 480))
            }
            objects.append(obj)
        
        return objects
    
    def _create_fallback_reading(self, sensor_type: SensorType) -> SensorReading:
        """åˆ›å»ºå¤‡ç”¨è¯»æ•°"""
        return SensorReading(
            sensor_type=sensor_type,
            data=None,
            timestamp=self._get_timestamp(),
            confidence=0.1,
            metadata={'error': 'sensor_unavailable'}
        )
    
    async def fuse_perceptions(self) -> Dict[str, Any]:
        """èåˆå¤šæ¨¡æ€æ„ŸçŸ¥"""
        if not self.perception_fusion:
            return {'error': 'fusion_not_available'}
        
        # è·å–æœ€æ–°æ„ŸçŸ¥æ•°æ®
        latest_readings = {}
        for sensor_type in SensorType:
            if self.sensor_buffers[sensor_type]:
                latest_readings[sensor_type] = self.sensor_buffers[sensor_type][-1]
        
        # æ‰§è¡Œæ„ŸçŸ¥èåˆ
        fused_perception = await self.perception_fusion(latest_readings)
        
        return fused_perception
    
    async def _simple_fusion(self, readings: Dict[SensorType, SensorReading]) -> Dict[str, Any]:
        """ç®€åŒ–æ„ŸçŸ¥èåˆ"""
        fusion_result = {
            'timestamp': self._get_timestamp(),
            'modalities_available': list(readings.keys()),
            'confidence_scores': {},
            'fused_state': {},
            'conflicts': [],
            'consensus': {}
        }
        
        # è®¡ç®—èåˆç½®ä¿¡åº¦
        for sensor_type, reading in readings.items():
            fusion_result['confidence_scores'][sensor_type.value] = reading.confidence
        
        # æ£€æµ‹å†²çª
        if SensorType.VISION in readings and SensorType.TOUCH in readings:
            # ç®€å•çš„å†²çªæ£€æµ‹é€»è¾‘
            fusion_result['conflicts'].append({
                'type': 'vision_touch_mismatch',
                'description': 'è§†è§‰å’Œè§¦è§‰ä¿¡æ¯å­˜åœ¨å·®å¼‚'
            })
        
        # è®¡ç®—å…±è¯†
        if len(readings) > 1:
            avg_confidence = np.mean([r.confidence for r in readings.values()])
            fusion_result['consensus']['overall_confidence'] = avg_confidence
        
        # èåˆçŠ¶æ€
        fusion_result['fused_state'] = {
            'environment_type': 'mixed_modal',
            'objects_detected': len(readings.get(SensorType.VISION, {}).get('data', {}).get('objects', [])),
            'audio_level': readings.get(SensorType.AUDIO, {}).get('data', {}).get('features', {}).get('rms_energy', 0),
            'touch_pressure': np.mean(readings.get(SensorType.TOUCH, {}).get('data', {}).get('touch_data', {}).get('pressure', [0])),
            'text_input': readings.get(SensorType.TEXT, {}).get('data', {}).get('text', '')
        }
        
        return fusion_result
    
    def _get_timestamp(self) -> float:
        """è·å–æ—¶é—´æˆ³"""
        import time
        return time.time()


class MotorController(nn.Module):
    """è¿åŠ¨æ§åˆ¶å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config
        
        # åŠ¨ä½œç©ºé—´
        self.action_space = config.get('action_space', {
            'linear_velocity': [-2.0, 2.0],
            'angular_velocity': [-2.0, 2.0],
            'gripper_position': [0.0, 1.0],
            'joint_positions': [0.0, 1.0]
        })
        
        # æ§åˆ¶å™¨ç±»å‹
        self.controller_type = config.get('type', 'pid')
        
        # PIDæ§åˆ¶å™¨å‚æ•°
        self.pid_gains = config.get('pid_gains', {
            'kp': 1.0, 'ki': 0.1, 'kd': 0.05
        })
        
        # å¹³è¡¡æ§åˆ¶å™¨
        self.balance_controller = config.get('balance_control', True)
        
        # å®‰å…¨çº¦æŸ
        self.safety_constraints = config.get('safety_constraints', {
            'max_velocity': 5.0,
            'max_acceleration': 2.0,
            'force_limit': 100.0,
            'torque_limit': 50.0
        })
        
        # è¿åŠ¨é¢„æµ‹æ¨¡å‹
        self.motion_predictor = self._build_motion_predictor()
    
    def _build_motion_predictor(self) -> nn.Module:
        """æ„å»ºè¿åŠ¨é¢„æµ‹æ¨¡å‹"""
        class MotionPredictor(nn.Module):
            def __init__(self):
                super().__init__()
                self.lstm = nn.LSTM(10, 32, 2, batch_first=True)
                self.fc = nn.Linear(32, 6)  # é¢„æµ‹ä½ç½®ã€é€Ÿåº¦ã€åŠ é€Ÿåº¦
            
            def forward(self, x):
                lstm_out, _ = self.lstm(x)
                prediction = self.fc(lstm_out[:, -1, :])
                return prediction
        
        return MotionPredictor()
    
    def forward(self, current_state: BodyState, target_state: BodyState, dt: float) -> Dict[str, Any]:
        """å‰å‘ä¼ æ’­æ‰§è¡Œè¿åŠ¨æ§åˆ¶"""
        # è¿åŠ¨è§„åˆ’
        trajectory = self._plan_trajectory(current_state, target_state, dt)
        
        # å®‰å…¨æ£€æŸ¥
        safe_trajectory = self._apply_safety_constraints(trajectory)
        
        # æ‰§è¡Œæ§åˆ¶
        control_commands = self._generate_control_commands(safe_trajectory, current_state)
        
        return {
            'trajectory': safe_trajectory,
            'control_commands': control_commands,
            'safety_status': self._check_safety_status(control_commands),
            'balance_metrics': self._calculate_balance_metrics(current_state, control_commands)
        }
    
    def _plan_trajectory(self, current_state: BodyState, target_state: BodyState, dt: float) -> Dict[str, Any]:
        """è§„åˆ’è¿åŠ¨è½¨è¿¹"""
        # è®¡ç®—ä½ç½®å·®
        pos_diff = target_state.position - current_state.position
        
        # ç”Ÿæˆäº”æ¬¡å¤šé¡¹å¼è½¨è¿¹
        trajectory = self._generate_quintic_trajectory(pos_diff, dt)
        
        # æ·»åŠ å¹³æ»‘å¤„ç†
        trajectory = self._smooth_trajectory(trajectory)
        
        return trajectory
    
    def _generate_quintic_trajectory(self, target_pos: np.ndarray, duration: float) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆäº”æ¬¡å¤šé¡¹å¼è½¨è¿¹"""
        # æ—¶é—´æ•°ç»„
        t = np.linspace(0, duration, int(duration / 0.01))
        
        # äº”æ¬¡å¤šé¡¹å¼ç³»æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # p(t) = a0 + a1*t + a2*t^2 + a3*t^3 + a4*t^4 + a5*t^5
        a0 = 0.0
        a1 = 0.0
        a2 = 0.0
        a3 = 10.0 / (duration**3)
        a4 = -15.0 / (duration**4)
        a5 = 6.0 / (duration**5)
        
        # è½¨è¿¹è®¡ç®—
        trajectory = {}
        for i, coord in enumerate(['x', 'y', 'z']):
            if i < len(target_pos):
                position = a0 + a1*t + a2*(t**2) + a3*(t**3) + a4*(t**4) + a5*(t**5)
                velocity = a1 + 2*a2*t + 3*a3*(t**2) + 4*a4*(t**3) + 5*a5*(t**4)
                acceleration = 2*a2 + 6*a3*t + 12*a4*(t**2) + 20*a5*(t**3)
                
                trajectory[coord] = {
                    'position': position * target_pos[i],
                    'velocity': velocity * target_pos[i],
                    'acceleration': acceleration * target_pos[i]
                }
        
        return trajectory
    
    def _smooth_trajectory(self, trajectory: Dict[str, Any]) -> Dict[str, Any]:
        """è½¨è¿¹å¹³æ»‘å¤„ç†"""
        smoothed_trajectory = {}
        
        for coord, data in trajectory.items():
            # ç®€å•ç§»åŠ¨å¹³å‡
            window_size = 5
            position = np.array(data['position'])
            velocity = np.array(data['velocity'])
            acceleration = np.array(data['acceleration'])
            
            # ç§»åŠ¨å¹³å‡
            smoothed_position = np.convolve(position, np.ones(window_size)/window_size, mode='same')
            smoothed_velocity = np.convolve(velocity, np.ones(window_size)/window_size, mode='same')
            smoothed_acceleration = np.convolve(acceleration, np.ones(window_size)/window_size, mode='same')
            
            smoothed_trajectory[coord] = {
                'position': smoothed_position,
                'velocity': smoothed_velocity,
                'acceleration': smoothed_acceleration
            }
        
        return smoothed_trajectory
    
    def _apply_safety_constraints(self, trajectory: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨å®‰å…¨çº¦æŸ"""
        safe_trajectory = trajectory.copy()
        
        # æ£€æŸ¥é€Ÿåº¦é™åˆ¶
        for coord, data in safe_trajectory.items():
            velocity = data['velocity']
            max_vel = self.safety_constraints['max_velocity']
            
            # é™é€Ÿ
            velocity = np.clip(velocity, -max_vel, max_vel)
            data['velocity'] = velocity
            
            # é‡æ–°è®¡ç®—åŠ é€Ÿåº¦
            if len(velocity) > 1:
                data['acceleration'] = np.gradient(velocity)
        
        return safe_trajectory
    
    def _generate_control_commands(self, trajectory: Dict[str, Any], current_state: BodyState) -> Dict[str, Any]:
        """ç”Ÿæˆæ§åˆ¶å‘½ä»¤"""
        commands = {
            'motor_commands': {},
            'gripper_commands': {},
            'joint_commands': {}
        }
        
        # ä¸ºæ¯ä¸ªåæ ‡è½´ç”Ÿæˆç”µæœºå‘½ä»¤
        for coord, data in trajectory.items():
            if coord in ['x', 'y', 'z']:
                # ç®€åŒ–çš„PIDæ§åˆ¶
                target_velocity = data['velocity'][0] if len(data['velocity']) > 0 else 0.0
                current_velocity = getattr(current_state.velocity, coord, 0.0)
                
                # PIDæ§åˆ¶å¾‹
                error = target_velocity - current_velocity
                kp = self.pid_gains['kp']
                ki = self.pid_gains['ki']
                kd = self.pid_gains['kd']
                
                # ç®€åŒ–çš„PIDè®¡ç®—
                motor_command = kp * error
                
                commands['motor_commands'][f'{coord}_velocity'] = motor_command
        
        # å…³èŠ‚æ§åˆ¶å‘½ä»¤
        joint_count = len(current_state.joint_angles)
        for i in range(joint_count):
            commands['joint_commands'][f'joint_{i}'] = {
                'position': current_state.joint_angles[i],
                'velocity': current_state.joint_velocities[i]
            }
        
        return commands
    
    def _check_safety_status(self, commands: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æŸ¥å®‰å…¨çŠ¶æ€"""
        safety_status = {
            'is_safe': True,
            'violations': [],
            'warnings': []
        }
        
        # æ£€æŸ¥ç”µæœºé€Ÿåº¦
        motor_commands = commands.get('motor_commands', {})
        for cmd_name, cmd_value in motor_commands.items():
            if 'velocity' in cmd_name:
                if abs(cmd_value) > self.safety_constraints['max_velocity']:
                    safety_status['is_safe'] = False
                    safety_status['violations'].append(f'{cmd_name} exceeds max velocity')
        
        # æ£€æŸ¥åŠ›çŸ©é™åˆ¶
        if safety_status['violations']:
            safety_status['warnings'].append('Performance degraded due to safety constraints')
        
        return safety_status
    
    def _calculate_balance_metrics(self, current_state: BodyState, commands: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—å¹³è¡¡æŒ‡æ ‡"""
        metrics = {
            'center_of_mass_stability': 1.0,  # ç®€åŒ–è®¡ç®—
            'support_polygon_margin': 0.5,
            'angular_momentum': 0.0,
            'balance_score': 0.8
        }
        
        if self.balance_controller:
            # å¢å¼ºçš„å¹³è¡¡æ§åˆ¶
            balance_error = np.linalg.norm(current_state.velocity)
            metrics['balance_score'] = max(0.0, 1.0 - balance_error / 10.0)
            metrics['balance_error'] = balance_error
        
        return metrics


class ActionPlanner:
    """åŠ¨ä½œè§„åˆ’å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # è§„åˆ’å‚æ•°
        self.horizon = config.get('horizon', 10)
        self.replanning_rate = config.get('replanning_rate', 0.1)
        self.uncertainty_handling = config.get('uncertainty_handling', 'monte_carlo')
        self.safety_constraints = config.get('safety_constraints', True)
        
        # ç›®æ ‡è·Ÿè¸ª
        self.current_goals = []
        self.goal_history = []
        
        # è§„åˆ’å†å²
        self.plan_history = deque(maxlen=100)
        
        self.logger.info("ğŸ¯ åŠ¨ä½œè§„åˆ’å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def plan_action(self, current_state: BodyState, perception_fusion: Dict[str, Any], 
                         goals: List[str]) -> Dict[str, Any]:
        """è§„åˆ’åŠ¨ä½œ"""
        # æ›´æ–°ç›®æ ‡
        self._update_goals(goals)
        
        # ç”Ÿæˆå€™é€‰åŠ¨ä½œ
        candidate_actions = await self._generate_candidate_actions(current_state, perception_fusion)
        
        # è¯„ä¼°åŠ¨ä½œ
        action_evaluations = await self._evaluate_actions(candidate_actions, current_state, goals)
        
        # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        best_action = self._select_best_action(action_evaluations)
        
        # ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        execution_plan = await self._generate_execution_plan(best_action, current_state)
        
        # å­˜å‚¨è§„åˆ’å†å²
        self.plan_history.append({
            'action': best_action,
            'evaluation': action_evaluations[best_action['type']],
            'timestamp': self._get_timestamp()
        })
        
        return execution_plan
    
    def _update_goals(self, goals: List[str]):
        """æ›´æ–°ç›®æ ‡"""
        # æ·»åŠ æ–°ç›®æ ‡
        for goal in goals:
            if goal not in [g['goal'] for g in self.current_goals]:
                self.current_goals.append({
                    'goal': goal,
                    'priority': 1.0,
                    'deadline': self._get_timestamp() + 300,  # 5åˆ†é’Ÿ
                    'status': 'active'
                })
        
        # æ¸…ç†è¿‡æœŸç›®æ ‡
        current_time = self._get_timestamp()
        self.current_goals = [g for g in self.current_goals if g['deadline'] > current_time]
        
        # æ›´æ–°ç›®æ ‡å†å²
        for goal in self.current_goals:
            if goal not in self.goal_history:
                self.goal_history.append(goal)
    
    async def _generate_candidate_actions(self, current_state: BodyState, 
                                         perception_fusion: Dict[str, Any]) -> List[ActionCommand]:
        """ç”Ÿæˆå€™é€‰åŠ¨ä½œ"""
        candidate_actions = []
        
        # åŸºäºå½“å‰çŠ¶æ€ç”ŸæˆåŸºç¡€åŠ¨ä½œ
        base_actions = [
            ActionCommand(
                action_type=ActionType.MOTOR_ACTION,
                parameters={'move_to': 'forward', 'distance': 1.0},
                duration=2.0,
                priority=5
            ),
            ActionCommand(
                action_type=ActionType.MOTOR_ACTION,
                parameters={'rotate': 'left', 'angle': 0.5},
                duration=1.0,
                priority=4
            ),
            ActionCommand(
                action_type=ActionType.MANIPULATION,
                parameters={'grasp_object': 'target'},
                duration=3.0,
                priority=6
            ),
            ActionCommand(
                action_type=ActionType.COMMUNICATION,
                parameters={'speak': 'hello', 'gesture': True},
                duration=2.0,
                priority=3
            )
        ]
        
        candidate_actions.extend(base_actions)
        
        # åŸºäºæ„ŸçŸ¥èåˆç”Ÿæˆé€‚åº”åŠ¨ä½œ
        if perception_fusion.get('fused_state', {}).get('objects_detected', 0) > 0:
            candidate_actions.append(ActionCommand(
                action_type=ActionType.COGNITIVE_ACTION,
                parameters={'observe_objects': True, 'focus_attention': 'objects'},
                duration=1.0,
                priority=7
            ))
        
        # åŸºäºæ–‡æœ¬è¾“å…¥ç”Ÿæˆäº¤æµåŠ¨ä½œ
        text_input = perception_fusion.get('fused_state', {}).get('text_input', '')
        if text_input:
            candidate_actions.append(ActionCommand(
                action_type=ActionType.COMMUNICATION,
                parameters={'respond_to_text': text_input},
                duration=3.0,
                priority=8
            ))
        
        return candidate_actions
    
    async def _evaluate_actions(self, candidate_actions: List[ActionCommand], 
                               current_state: BodyState, goals: List[str]) -> Dict[str, ActionCommand]:
        """è¯„ä¼°åŠ¨ä½œ"""
        evaluations = {}
        
        for action in candidate_actions:
            score = 0.0
            
            # ç›®æ ‡åŒ¹é…åº¦
            goal_match = self._calculate_goal_match(action, goals)
            score += goal_match * 0.4
            
            # å®‰å…¨æ€§è¯„ä¼°
            safety_score = self._assess_safety(action, current_state)
            score += safety_score * 0.3
            
            # æ•ˆç‡è¯„ä¼°
            efficiency_score = self._assess_efficiency(action, current_state)
            score += efficiency_score * 0.2
            
            # å¯è¡Œæ€§è¯„ä¼°
            feasibility_score = self._assess_feasibility(action, current_state)
            score += feasibility_score * 0.1
            
            evaluations[action.action_type.value] = {
                'action': action,
                'total_score': score,
                'goal_match': goal_match,
                'safety_score': safety_score,
                'efficiency_score': efficiency_score,
                'feasibility_score': feasibility_score
            }
        
        return evaluations
    
    def _calculate_goal_match(self, action: ActionCommand, goals: List[str]) -> float:
        """è®¡ç®—ç›®æ ‡åŒ¹é…åº¦"""
        # ç®€åŒ–çš„ç›®æ ‡åŒ¹é…åº¦è®¡ç®—
        action_keywords = []
        for param_value in action.parameters.values():
            if isinstance(param_value, str):
                action_keywords.append(param_value.lower())
        
        match_count = 0
        for goal in goals:
            goal_words = goal.lower().split()
            for word in goal_words:
                if any(keyword in word for keyword in action_keywords):
                    match_count += 1
        
        return min(1.0, match_count / max(1, len(goals)))
    
    def _assess_safety(self, action: ActionCommand, current_state: BodyState) -> float:
        """è¯„ä¼°å®‰å…¨æ€§"""
        safety_score = 1.0
        
        # æ£€æŸ¥åŠ¨ä½œå®‰å…¨æ€§
        if action.action_type == ActionType.MOTOR_ACTION:
            velocity = action.parameters.get('velocity', 0)
            if abs(velocity) > 2.0:
                safety_score *= 0.7
        
        # æ£€æŸ¥å½“å‰ä½ç½®çš„å®‰å…¨æ€§
        pos = current_state.position
        if np.linalg.norm(pos) < 0.5:  # æ¥è¿‘åŸç‚¹
            safety_score *= 0.8
        
        return safety_score
    
    def _assess_efficiency(self, action: ActionCommand, current_state: BodyState) -> float:
        """è¯„ä¼°æ•ˆç‡"""
        # åŸºäºåŠ¨ä½œæŒç»­æ—¶é—´å’Œç›®æ ‡è·ç¦»çš„æ•ˆç‡è¯„ä¼°
        duration = action.duration
        priority = action.priority
        
        # çŸ­æ—¶é—´é«˜ä¼˜å…ˆçº§çš„åŠ¨ä½œæ›´æœ‰æ•ˆ
        efficiency = priority / (duration + 1.0)
        return min(1.0, efficiency / 10.0)
    
    def _assess_feasibility(self, action: ActionCommand, current_state: BodyState) -> float:
        """è¯„ä¼°å¯è¡Œæ€§"""
        feasibility = 1.0
        
        # æ£€æŸ¥å½“å‰çŠ¶æ€æ˜¯å¦æ”¯æŒè¯¥åŠ¨ä½œ
        if action.action_type == ActionType.MANIPULATION:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¹³è¡¡æ€§
            balance_score = current_state.balance_metrics.get('balance_score', 0.5)
            feasibility *= balance_score
        
        return feasibility
    
    def _select_best_action(self, evaluations: Dict[str, Any]) -> ActionCommand:
        """é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ"""
        if not evaluations:
            return ActionCommand(
                action_type=ActionType.COGNITIVE_ACTION,
                parameters={'wait': True},
                duration=1.0,
                priority=1
            )
        
        # æŒ‰æ€»åˆ†æ’åº
        best_action_type = max(evaluations.keys(), 
                             key=lambda x: evaluations[x]['total_score'])
        
        return evaluations[best_action_type]['action']
    
    async def _generate_execution_plan(self, action: ActionCommand, current_state: BodyState) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
        execution_plan = {
            'primary_action': action,
            'execution_steps': [],
            'fallback_actions': [],
            'monitoring_points': [],
            'expected_outcome': {},
            'risk_assessment': {},
            'timestamp': self._get_timestamp()
        }
        
        # ç”Ÿæˆæ‰§è¡Œæ­¥éª¤
        execution_plan['execution_steps'] = self._decompose_action(action)
        
        # ç”Ÿæˆå¤‡ç”¨åŠ¨ä½œ
        execution_plan['fallback_actions'] = self._generate_fallback_actions(action)
        
        # è®¾ç½®ç›‘æ§ç‚¹
        execution_plan['monitoring_points'] = self._set_monitoring_points(action)
        
        # é¢„æœŸç»“æœ
        execution_plan['expected_outcome'] = self._predict_outcome(action, current_state)
        
        # é£é™©è¯„ä¼°
        execution_plan['risk_assessment'] = self._assess_risks(action, current_state)
        
        return execution_plan
    
    def _decompose_action(self, action: ActionCommand) -> List[Dict[str, Any]]:
        """åˆ†è§£åŠ¨ä½œ"""
        steps = []
        
        if action.action_type == ActionType.MOTOR_ACTION:
            # åˆ†è§£ä¸ºç§»åŠ¨æ­¥éª¤
            steps = [
                {'step': 'prepare_movement', 'duration': 0.2, 'parameters': {}},
                {'step': 'execute_movement', 'duration': action.duration - 0.4, 'parameters': action.parameters},
                {'step': 'stabilize', 'duration': 0.2, 'parameters': {}}
            ]
        
        elif action.action_type == ActionType.MANIPULATION:
            # åˆ†è§£ä¸ºæ“ä½œæ­¥éª¤
            steps = [
                {'step': 'approach_target', 'duration': 1.0, 'parameters': {'target': 'object'}},
                {'step': 'grasp_action', 'duration': 1.5, 'parameters': {'gripper': 'close'}},
                {'step': 'hold_position', 'duration': action.duration - 2.5, 'parameters': {}},
                {'step': 'release', 'duration': 0.5, 'parameters': {'gripper': 'open'}}
            ]
        
        elif action.action_type == ActionType.COMMUNICATION:
            # åˆ†è§£ä¸ºäº¤æµæ­¥éª¤
            steps = [
                {'step': 'prepare_communication', 'duration': 0.5, 'parameters': {}},
                {'step': 'execute_speech', 'duration': action.duration - 1.0, 'parameters': action.parameters},
                {'step': 'end_communication', 'duration': 0.5, 'parameters': {}}
            ]
        
        else:
            # é»˜è®¤æ­¥éª¤
            steps = [
                {'step': 'start_action', 'duration': 0.1, 'parameters': action.parameters},
                {'step': 'maintain_action', 'duration': action.duration - 0.2, 'parameters': {}},
                {'step': 'end_action', 'duration': 0.1, 'parameters': {}}
            ]
        
        return steps
    
    def _generate_fallback_actions(self, action: ActionCommand) -> List[ActionCommand]:
        """ç”Ÿæˆå¤‡ç”¨åŠ¨ä½œ"""
        fallback_actions = []
        
        if action.action_type == ActionType.MOTOR_ACTION:
            # å¦‚æœç§»åŠ¨å¤±è´¥ï¼Œæ”¹ä¸ºåŸåœ°ç­‰å¾…
            fallback_actions.append(ActionCommand(
                action_type=ActionType.COGNITIVE_ACTION,
                parameters={'wait': True},
                duration=2.0,
                priority=2
            ))
        
        elif action.action_type == ActionType.MANIPULATION:
            # å¦‚æœæŠ“å–å¤±è´¥ï¼Œæ”¹ä¸ºè§‚å¯Ÿ
            fallback_actions.append(ActionCommand(
                action_type=ActionType.COGNITIVE_ACTION,
                parameters={'observe_environment': True},
                duration=3.0,
                priority=3
            ))
        
        return fallback_actions
    
    def _set_monitoring_points(self, action: ActionCommand) -> List[Dict[str, Any]]:
        """è®¾ç½®ç›‘æ§ç‚¹"""
        monitoring_points = []
        
        # åœ¨åŠ¨ä½œæ‰§è¡Œçš„25%ã€50%ã€75%è®¾ç½®ç›‘æ§ç‚¹
        total_duration = action.duration
        for percentage in [0.25, 0.5, 0.75]:
            time_point = total_duration * percentage
            monitoring_points.append({
                'time': time_point,
                'checks': ['position', 'velocity', 'safety'],
                'thresholds': {
                    'position_error': 0.1,
                    'velocity_error': 0.5,
                    'safety_margin': 0.2
                }
            })
        
        return monitoring_points
    
    def _predict_outcome(self, action: ActionCommand, current_state: BodyState) -> Dict[str, Any]:
        """é¢„æµ‹åŠ¨ä½œç»“æœ"""
        outcome = {
            'success_probability': 0.8,
            'expected_duration': action.duration,
            'resource_usage': {
                'energy': action.duration * 0.1,
                'computational': 0.2
            },
            'side_effects': []
        }
        
        # æ ¹æ®åŠ¨ä½œç±»å‹é¢„æµ‹å…·ä½“ç»“æœ
        if action.action_type == ActionType.MOTOR_ACTION:
            outcome['position_change'] = 'é¢„è®¡ç§»åŠ¨1-2ç±³'
            outcome['side_effects'] = ['æ¶ˆè€—èƒ½é‡', 'ä½ç½®æ”¹å˜']
        
        elif action.action_type == ActionType.MANIPULATION:
            outcome['object_interaction'] = 'é¢„è®¡ä¸ç›®æ ‡ç‰©ä½“äº¤äº’'
            outcome['side_effects'] = ['ç‰©ä½“ä½ç§»', 'åŠ›çš„ä½œç”¨']
        
        return outcome
    
    def _assess_risks(self, action: ActionCommand, current_state: BodyState) -> Dict[str, Any]:
        """è¯„ä¼°é£é™©"""
        risk_assessment = {
            'overall_risk': 'low',
            'specific_risks': [],
            'mitigation_strategies': [],
            'risk_score': 0.2
        }
        
        specific_risks = []
        mitigation_strategies = []
        
        if action.action_type == ActionType.MOTOR_ACTION:
            risk = self._assess_movement_risk(action, current_state)
            specific_risks.extend(risk)
            mitigation_strategies.append('æ¸è¿›å¼ç§»åŠ¨')
        
        if action.action_type == ActionType.MANIPULATION:
            risk = self._assess_manipulation_risk(action, current_state)
            specific_risks.extend(risk)
            mitigation_strategies.append('ç²¾ç¡®æ§åˆ¶')
        
        risk_assessment['specific_risks'] = specific_risks
        risk_assessment['mitigation_strategies'] = mitigation_strategies
        
        # è®¡ç®—æ€»ä½“é£é™©åˆ†æ•°
        risk_score = len(specific_risks) * 0.1
        risk_assessment['risk_score'] = min(1.0, risk_score)
        
        if risk_score > 0.7:
            risk_assessment['overall_risk'] = 'high'
        elif risk_score > 0.4:
            risk_assessment['overall_risk'] = 'medium'
        
        return risk_assessment
    
    def _assess_movement_risk(self, action: ActionCommand, current_state: BodyState) -> List[str]:
        """è¯„ä¼°ç§»åŠ¨é£é™©"""
        risks = []
        
        if 'velocity' in action.parameters:
            velocity = action.parameters['velocity']
            if abs(velocity) > 1.5:
                risks.append('é«˜é€Ÿç§»åŠ¨é£é™©')
        
        if 'distance' in action.parameters:
            distance = action.parameters['distance']
            if distance > 2.0:
                risks.append('é•¿è·ç¦»ç§»åŠ¨é£é™©')
        
        return risks
    
    def _assess_manipulation_risk(self, action: ActionCommand, current_state: BodyState) -> List[str]:
        """è¯„ä¼°æ“ä½œé£é™©"""
        risks = []
        
        if current_state.balance_metrics.get('balance_score', 1.0) < 0.7:
            risks.append('å¹³è¡¡ä¸è¶³é£é™©')
        
        return risks
    
    def _get_timestamp(self) -> float:
        """è·å–æ—¶é—´æˆ³"""
        import time
        return time.time()


class EmbodiedIntelligence:
    """
    å…·èº«æ™ºèƒ½ä¸»ç±»
    
    æ•´åˆæ‰€æœ‰äº¤äº’è¡ŒåŠ¨åŠŸèƒ½ï¼š
    - å¤šæ¨¡æ€æ„ŸçŸ¥
    - è¿åŠ¨æ§åˆ¶
    - åŠ¨ä½œè§„åˆ’
    - ç¯å¢ƒäº¤äº’
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å…·èº«æ™ºèƒ½ç³»ç»Ÿ
        
        Args:
            config: ç³»ç»Ÿé…ç½®
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # èº«ä½“æ¨¡å‹é…ç½®
        self.body_model = config.get('body_model', 'humanoid')
        self.motor_control = config.get('motor_control', 'policy_gradient')
        self.sensory_fusion = config.get('sensory_fusion', 'kalman_filter')
        self.balance_control = config.get('balance_control', True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.multimodal_perception = None
        self.motor_controller = None
        self.action_planner = None
        
        # èº«ä½“çŠ¶æ€
        self.current_body_state = self._create_initial_body_state()
        
        # äº¤äº’äº‹ä»¶å†å²
        self.interaction_history = deque(maxlen=500)
        
        # æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics = {
            'actions_executed': 0,
            'successful_interactions': 0,
            'safety_violations': 0,
            'efficiency_score': 0.8
        }
        
        self.logger.info("ğŸ¤– å…·èº«æ™ºèƒ½ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–å…·èº«æ™ºèƒ½ç»„ä»¶"""
        self.logger.info("ğŸ”§ åˆå§‹åŒ–å…·èº«æ™ºèƒ½ç»„ä»¶...")
        
        try:
            # åˆå§‹åŒ–å¤šæ¨¡æ€æ„ŸçŸ¥
            perception_config = self.config.get('multimodal_perception', {})
            self.multimodal_perception = MultimodalPerception(perception_config)
            await self.multimodal_perception.initialize()
            
            # åˆå§‹åŒ–è¿åŠ¨æ§åˆ¶å™¨
            motor_config = self.config.get('motor_control_config', {})
            self.motor_controller = MotorController(motor_config)
            
            # åˆå§‹åŒ–åŠ¨ä½œè§„åˆ’å™¨
            planning_config = self.config.get('action_planning', {})
            self.action_planner = ActionPlanner(planning_config)
            
            self.logger.info("âœ… å…·èº«æ™ºèƒ½ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ å…·èº«æ™ºèƒ½ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _create_initial_body_state(self) -> BodyState:
        """åˆ›å»ºåˆå§‹èº«ä½“çŠ¶æ€"""
        return BodyState(
            position=np.array([0.0, 0.0, 0.0]),
            velocity=np.array([0.0, 0.0, 0.0]),
            orientation=np.array([0.0, 0.0, 0.0, 1.0]),  # å››å…ƒæ•°
            joint_angles=np.random.uniform(-0.5, 0.5, 7),  # 7ä¸ªå…³èŠ‚
            joint_velocities=np.zeros(7),
            force_sensors={'left_foot': 50.0, 'right_foot': 50.0},
            balance_metrics={
                'center_of_mass': np.array([0.0, 0.0, 0.0]),
                'support_polygon_area': 0.1,
                'balance_score': 0.8,
                'angular_momentum': 0.0
            }
        )
    
    async def perceive_environment(self) -> Dict[str, Any]:
        """æ„ŸçŸ¥ç¯å¢ƒ"""
        perception_tasks = [
            self.multimodal_perception.capture_vision(),
            self.multimodal_perception.capture_audio(),
            self.multimodal_perception.capture_touch(),
            self.multimodal_perception.capture_proprioception(self.current_body_state)
        ]
        
        # å¹¶è¡Œæ‰§è¡Œæ„ŸçŸ¥ä»»åŠ¡
        vision_reading, audio_reading, touch_reading, proprioception_reading = await asyncio.gather(*perception_tasks)
        
        # èåˆæ„ŸçŸ¥ç»“æœ
        fused_perception = await self.multimodal_perception.fuse_perceptions()
        
        return {
            'individual_readings': {
                'vision': vision_reading,
                'audio': audio_reading,
                'touch': touch_reading,
                'proprioception': proprioception_reading
            },
            'fused_perception': fused_perception,
            'timestamp': self._get_timestamp()
        }
    
    async def plan_action(self, goals: List[str]) -> Dict[str, Any]:
        """è§„åˆ’åŠ¨ä½œ"""
        # æ„ŸçŸ¥ç¯å¢ƒ
        perception_data = await self.perceive_environment()
        fused_perception = perception_data['fused_perception']
        
        # è§„åˆ’åŠ¨ä½œ
        execution_plan = await self.action_planner.plan_action(
            current_state=self.current_body_state,
            perception_fusion=fused_perception,
            goals=goals
        )
        
        return execution_plan
    
    async def execute_action(self, action_command: ActionCommand) -> Dict[str, Any]:
        """æ‰§è¡ŒåŠ¨ä½œ"""
        self.logger.info(f"ğŸ¬ æ‰§è¡ŒåŠ¨ä½œ: {action_command.action_type.value}")
        
        execution_result = {
            'success': False,
            'final_state': None,
            'performance_metrics': {},
            'interaction_events': [],
            'safety_checks': [],
            'timestamp': self._get_timestamp()
        }
        
        try:
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.performance_metrics['actions_executed'] += 1
            
            # æ ¹æ®åŠ¨ä½œç±»å‹æ‰§è¡Œ
            if action_command.action_type == ActionType.MOTOR_ACTION:
                execution_result = await self._execute_motor_action(action_command)
            elif action_command.action_type == ActionType.MANIPULATION:
                execution_result = await self._execute_manipulation(action_command)
            elif action_command.action_type == ActionType.COMMUNICATION:
                execution_result = await self._execute_communication(action_command)
            elif action_command.action_type == ActionType.COGNITIVE_ACTION:
                execution_result = await self._execute_cognitive_action(action_command)
            elif action_command.action_type == ActionType.SOCIAL_ACTION:
                execution_result = await self._execute_social_action(action_command)
            
            # æ›´æ–°èº«ä½“çŠ¶æ€
            if execution_result.get('final_state'):
                self.current_body_state = execution_result['final_state']
            
            # è®°å½•äº¤äº’äº‹ä»¶
            if execution_result.get('interaction_events'):
                self.interaction_history.extend(execution_result['interaction_events'])
            
            # æ›´æ–°æˆåŠŸæŒ‡æ ‡
            if execution_result.get('success'):
                self.performance_metrics['successful_interactions'] += 1
            
            execution_result['timestamp'] = self._get_timestamp()
            
        except Exception as e:
            self.logger.error(f"åŠ¨ä½œæ‰§è¡Œå¤±è´¥: {e}")
            execution_result['error'] = str(e)
        
        return execution_result
    
    async def _execute_motor_action(self, action: ActionCommand) -> Dict[str, Any]:
        """æ‰§è¡Œç”µæœºåŠ¨ä½œ"""
        # ç”Ÿæˆç›®æ ‡çŠ¶æ€
        target_state = self.current_body_state.copy()
        
        # æ ¹æ®åŠ¨ä½œå‚æ•°æ›´æ–°ç›®æ ‡çŠ¶æ€
        if 'move_to' in action.parameters:
            direction = action.parameters['move_to']
            distance = action.parameters.get('distance', 1.0)
            
            if direction == 'forward':
                target_state.position[1] += distance
            elif direction == 'backward':
                target_state.position[1] -= distance
            elif direction == 'left':
                target_state.position[0] -= distance
            elif direction == 'right':
                target_state.position[0] += distance
        
        elif 'rotate' in action.parameters:
            rotation = action.parameters['rotate']
            angle = action.parameters.get('angle', 0.5)
            
            # ç®€åŒ–æ—‹è½¬å®ç°
            if rotation == 'left':
                target_state.orientation = self._rotate_orientation(
                    target_state.orientation, -angle
                )
            elif rotation == 'right':
                target_state.orientation = self._rotate_orientation(
                    target_state.orientation, angle
                )
        
        # ä½¿ç”¨è¿åŠ¨æ§åˆ¶å™¨æ‰§è¡Œ
        dt = 0.01  # æ—¶é—´æ­¥é•¿
        control_output = self.motor_controller(
            current_state=self.current_body_state,
            target_state=target_state,
            dt=dt
        )
        
        # æ¨¡æ‹Ÿæ‰§è¡Œè¿‡ç¨‹
        await asyncio.sleep(action.duration)
        
        # æ›´æ–°çŠ¶æ€
        final_state = self.current_body_state.copy()
        final_state.position = target_state.position
        final_state.orientation = target_state.orientation
        
        return {
            'success': True,
            'final_state': final_state,
            'control_output': control_output,
            'performance_metrics': {
                'trajectory_deviation': 0.1,
                'execution_time': action.duration,
                'energy_consumption': action.duration * 0.1
            },
            'interaction_events': [
                InteractionEvent(
                    event_type='movement',
                    participants=['è®¤çŸ¥ä¸»ä½“'],
                    intensity=0.7,
                    timestamp=self._get_timestamp(),
                    outcome={'distance_moved': np.linalg.norm(target_state.position - self.current_body_state.position)}
                )
            ]
        }
    
    async def _execute_manipulation(self, action: ActionCommand) -> Dict[str, Any]:
        """æ‰§è¡Œæ“ä½œåŠ¨ä½œ"""
        # æ¨¡æ‹ŸæŠ“å–æ“ä½œ
        await asyncio.sleep(action.duration)
        
        # æ›´æ–°æŠ“å–å™¨çŠ¶æ€
        final_state = self.current_body_state.copy()
        if 'gripper' in action.parameters:
            if action.parameters['gripper'] == 'close':
                final_state.joint_angles[-1] = 1.0  # æŠ“å–å™¨é—­åˆ
            elif action.parameters['gripper'] == 'open':
                final_state.joint_angles[-1] = 0.0  # æŠ“å–å™¨å¼ å¼€
        
        return {
            'success': True,
            'final_state': final_state,
            'performance_metrics': {
                'manipulation_success': 0.9,
                'force_application': 5.0,
                'precision': 0.85
            },
            'interaction_events': [
                InteractionEvent(
                    event_type='manipulation',
                    participants=['è®¤çŸ¥ä¸»ä½“', 'object'],
                    intensity=0.8,
                    timestamp=self._get_timestamp(),
                    outcome={'object_grasped': True}
                )
            ]
        }
    
    async def _execute_communication(self, action: ActionCommand) -> Dict[str, Any]:
        """æ‰§è¡Œäº¤æµåŠ¨ä½œ"""
        # æ¨¡æ‹Ÿè¯­éŸ³è¾“å‡º
        if 'speak' in action.parameters:
            text = action.parameters['speak']
            self.logger.info(f"ğŸ—£ï¸  è¯´è¯: {text}")
        
        # æ¨¡æ‹Ÿæ‰‹åŠ¿
        if action.parameters.get('gesture', False):
            await self._perform_gesture()
        
        await asyncio.sleep(action.duration)
        
        return {
            'success': True,
            'final_state': self.current_body_state,
            'performance_metrics': {
                'speech_clarity': 0.9,
                'gesture_recognizability': 0.8,
                'communication_effectiveness': 0.85
            },
            'interaction_events': [
                InteractionEvent(
                    event_type='communication',
                    participants=['è®¤çŸ¥ä¸»ä½“', 'environment'],
                    intensity=0.6,
                    timestamp=self._get_timestamp(),
                    outcome={'message_delivered': True, 'text': action.parameters.get('speak', '')}
                )
            ]
        }
    
    async def _execute_cognitive_action(self, action: ActionCommand) -> Dict[str, Any]:
        """æ‰§è¡Œè®¤çŸ¥åŠ¨ä½œ"""
        # è®¤çŸ¥åŠ¨ä½œé€šå¸¸ä¸æ”¹å˜èº«ä½“çŠ¶æ€
        await asyncio.sleep(action.duration)
        
        return {
            'success': True,
            'final_state': self.current_body_state,
            'performance_metrics': {
                'cognitive_load': 0.3,
                'attention_focus': 'maintained',
                'processing_time': action.duration
            },
            'interaction_events': [
                InteractionEvent(
                    event_type='cognitive_processing',
                    participants=['è®¤çŸ¥ä¸»ä½“'],
                    intensity=0.4,
                    timestamp=self._get_timestamp(),
                    outcome={'cognitive_task_completed': True}
                )
            ]
        }
    
    async def _execute_social_action(self, action: ActionCommand) -> Dict[str, Any]:
        """æ‰§è¡Œç¤¾äº¤åŠ¨ä½œ"""
        # æ¨¡æ‹Ÿç¤¾äº¤äº¤äº’
        await asyncio.sleep(action.duration)
        
        return {
            'success': True,
            'final_state': self.current_body_state,
            'performance_metrics': {
                'social_engagement': 0.7,
                'empathy_response': 0.6,
                'cooperation_level': 0.8
            },
            'interaction_events': [
                InteractionEvent(
                    event_type='social_interaction',
                    participants=['è®¤çŸ¥ä¸»ä½“', 'other_è®¤çŸ¥ä¸»ä½“s'],
                    intensity=0.9,
                    timestamp=self._get_timestamp(),
                    outcome={'social_bond_strengthened': True}
                )
            ]
        }
    
    async def _perform_gesture(self):
        """æ‰§è¡Œæ‰‹åŠ¿"""
        # æ¨¡æ‹Ÿæ‰‹åŠ¿åŠ¨ç”»
        await asyncio.sleep(0.5)
    
    def _rotate_orientation(self, orientation: np.ndarray, angle: float) -> np.ndarray:
        """æ—‹è½¬æ–¹å‘ï¼ˆå››å…ƒæ•°ï¼‰"""
        # ç®€åŒ–çš„æ—‹è½¬å®ç°
        return orientation  # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„å››å…ƒæ•°è¿ç®—
    
    async def execute_cognitive_task(self, cognitive_state: Dict[str, Any], environment) -> Dict[str, Any]:
        """æ‰§è¡Œè®¤çŸ¥ä»»åŠ¡"""
        goals = ['complete_cognitive_task', 'maintain_focus']
        
        # è§„åˆ’å¹¶æ‰§è¡ŒåŠ¨ä½œ
        execution_plan = await self.plan_action(goals)
        action = execution_plan['primary_action']
        
        execution_result = await self.execute_action(action)
        
        return {
            'task_success': execution_result.get('success', False),
            'performance_score': execution_result.get('performance_metrics', {}),
            'embodied_response': execution_result,
            'cognitive_integration': {
                'attention_maintained': True,
                'motor_cognitive_sync': 0.8
            }
        }
    
    async def run_interaction_loop(self, max_iterations: int = 100) -> List[Dict[str, Any]]:
        """è¿è¡Œäº¤äº’å¾ªç¯"""
        interaction_log = []
        
        for iteration in range(max_iterations):
            try:
                # æ„ŸçŸ¥ç¯å¢ƒ
                perception = await self.perceive_environment()
                
                # ç”Ÿæˆç®€å•ç›®æ ‡
                goals = [f'interaction_goal_{iteration}']
                
                # è§„åˆ’åŠ¨ä½œ
                execution_plan = await self.plan_action(goals)
                action = execution_plan['primary_action']
                
                # æ‰§è¡ŒåŠ¨ä½œ
                execution_result = await self.execute_action(action)
                
                # è®°å½•äº¤äº’
                interaction_log.append({
                    'iteration': iteration,
                    'perception': perception,
                    'execution_plan': execution_plan,
                    'execution_result': execution_result,
                    'timestamp': self._get_timestamp()
                })
                
                # çŸ­æš‚ç­‰å¾…
                await asyncio.sleep(0.1)
                
            except Exception as e:
                self.logger.error(f"äº¤äº’å¾ªç¯é”™è¯¯ (iteration {iteration}): {e}")
                continue
        
        return interaction_log
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        # è®¡ç®—æˆåŠŸç‡
        success_rate = (self.performance_metrics['successful_interactions'] / 
                       max(1, self.performance_metrics['actions_executed']))
        
        # è®¡ç®—å®‰å…¨ç‡
        safety_rate = 1.0 - (self.performance_metrics['safety_violations'] / 
                            max(1, self.performance_metrics['actions_executed']))
        
        return {
            **self.performance_metrics,
            'success_rate': success_rate,
            'safety_rate': safety_rate,
            'interaction_count': len(self.interaction_history),
            'current_body_state': {
                'position': self.current_body_state.position.tolist(),
                'balance_score': self.current_body_state.balance_metrics.get('balance_score', 0.8)
            }
        }
    
    def _get_timestamp(self) -> float:
        """è·å–æ—¶é—´æˆ³"""
        import time
        return time.time()
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("ğŸ§¹ æ¸…ç†å…·èº«æ™ºèƒ½ç³»ç»Ÿèµ„æº...")
        
        # æ¸…ç†äº¤äº’å†å²
        self.interaction_history.clear()
        
        # æ¸…ç†æ„ŸçŸ¥ç¼“å†²
        if self.multimodal_perception:
            for sensor_type in self.multimodal_perception.sensor_buffers:
                self.multimodal_perception.sensor_buffers[sensor_type].clear()
        
        self.logger.info("âœ… å…·èº«æ™ºèƒ½ç³»ç»Ÿèµ„æºæ¸…ç†å®Œæˆ")